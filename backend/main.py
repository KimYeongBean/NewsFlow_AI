# ================================
# 0. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ================================
import feedparser  # RSS í”¼ë“œë¥¼ íŒŒì‹±(ë¶„ì„)í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import time  # í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì ì‹œ ë©ˆì¶”ê±°ë‚˜(sleep) ì‹œê°„ ê´€ë ¨ ì‘ì—…ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import os  # ìš´ì˜ì²´ì œì™€ ìƒí˜¸ì‘ìš©í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (í´ë” ìƒì„±, íŒŒì¼ ê²½ë¡œ ë“±)
from datetime import datetime, timedelta  # ë‚ ì§œì™€ ì‹œê°„ì„ ë‹¤ë£¨ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from urllib.parse import quote  # URLì— í•œê¸€ ê°™ì€ ë¬¸ìë¥¼ ì•ˆì „í•˜ê²Œ í¬í•¨ì‹œí‚¤ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from openai import AzureOpenAI  # Azureì˜ OpenAI ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import re  # ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•´ ë¬¸ìì—´ì—ì„œ íŠ¹ì • íŒ¨í„´ì„ ì°¾ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import requests  # ì›¹ì‚¬ì´íŠ¸ì— HTTP ìš”ì²­ì„ ë³´ë‚´ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import uuid  # ê³ ìœ í•œ IDë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ë²ˆì—­ ìš”ì²­ ì‹œ ì¶”ì  IDë¡œ ì‚¬ìš©)
from bs4 import BeautifulSoup  # HTML ë° XML íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ì‰½ê²Œ ì¶”ì¶œí•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from selenium import webdriver  # ì›¹ ë¸Œë¼ìš°ì €ë¥¼ ìë™ìœ¼ë¡œ ì œì–´í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from selenium.webdriver.chrome.service import Service  # ì…€ë ˆë‹ˆì›€ì—ì„œ í¬ë¡¬ ë“œë¼ì´ë²„ ì„œë¹„ìŠ¤ë¥¼ ê´€ë¦¬
from selenium.webdriver.chrome.options import Options  # í¬ë¡¬ ë¸Œë¼ìš°ì €ì˜ ì˜µì…˜(ì˜ˆ: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ)ì„ ì„¤ì •
from selenium.webdriver.support.ui import WebDriverWait  # ì…€ë ˆë‹ˆì›€ì—ì„œ íŠ¹ì • ì¡°ê±´ì´ ë§Œì¡±ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ë„ë¡ ì„¤ì •

# ================================
# 1. ì‚¬ìš©ì ì„¤ì •
# ================================
# GPTê°€ ì‹ ë¢°ë„ë¥¼ í‰ê°€í•  ë•Œ ê¸°ì¤€ìœ¼ë¡œ ì‚¼ì„ ì£¼ìš” ì–¸ë¡ ì‚¬ ëª©ë¡
user_selected_sources = ["ì¡°ì„ ì¼ë³´", "í•œê²¨ë ˆ", "ì¤‘ì•™ì¼ë³´", "ë™ì•„ì¼ë³´", "ê²½í–¥ì‹ ë¬¸"]
# ì‚¬ìš©ìê°€ êµ¬ë…í•˜ì—¬ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•  ë©”ì¸ ì¹´í…Œê³ ë¦¬ ëª©ë¡
user_follow_categories = ["ì—¬í–‰"]

# ================================
# 2. Azure AI ì„œë¹„ìŠ¤ ì„¤ì •
# ================================
# --- Azure AI ë²ˆì—­ê¸°(Translator) ì„¤ì • ---
translator_key = "..."  # Azure ë²ˆì—­ ì„œë¹„ìŠ¤ì˜ êµ¬ë… í‚¤
translator_endpoint = "https://api.cognitive.microsofttranslator.com/"  # Azure ë²ˆì—­ ì„œë¹„ìŠ¤ì˜ ì—”ë“œí¬ì¸íŠ¸ ì£¼ì†Œ
translator_location = "KoreaCentral"  # Azure ë²ˆì—­ ì„œë¹„ìŠ¤ì˜ ë¦¬ì†ŒìŠ¤ ì§€ì—­

# ================================
# 3. Azure OpenAI ì´ˆê¸°í™”
# ================================
endpoint = "https://newscheck2.openai.azure.com/"  # Azure OpenAI ì„œë¹„ìŠ¤ì˜ ì—”ë“œí¬ì¸íŠ¸ ì£¼ì†Œ
deployment = "gpt-4o"  # Azureì— ë°°í¬í•œ ëª¨ë¸ì˜ 'ë°°í¬ ì´ë¦„'
subscription_key = "..."  # Azure OpenAI ì„œë¹„ìŠ¤ì˜ êµ¬ë… í‚¤
api_version = "2025-01-01-preview"  # ì‚¬ìš©í•  APIì˜ ë²„ì „

# ìœ„ ì„¤ì •ê°’ë“¤ì„ ì‚¬ìš©í•˜ì—¬ Azure OpenAI ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ìˆëŠ” í´ë¼ì´ì–¸íŠ¸ ê°ì²´ë¥¼ ìƒì„±
client = AzureOpenAI(
    azure_endpoint=endpoint,
    api_key=subscription_key,
    api_version=api_version,
)

# ================================
# 4. ì „ì²´ ë‰´ìŠ¤/ì¹´í…Œê³ ë¦¬
# ================================
# ìˆ˜ì§‘ ëŒ€ìƒì´ ë  ì „ì²´ ì–¸ë¡ ì‚¬ ëª©ë¡
all_sources = [
    'MBCë‰´ìŠ¤', 'ì—°í•©ë‰´ìŠ¤', 'ì¡°ì„ ì¼ë³´', 'ë‰´ìŠ¤1', 'JTBC ë‰´ìŠ¤',
    'ì¤‘ì•™ì¼ë³´', 'SBS ë‰´ìŠ¤', 'YTN', 'í•œê²¨ë ˆ', 'ê²½í–¥ì‹ ë¬¸',
    'ì˜¤ë§ˆì´ë‰´ìŠ¤', 'í•œêµ­ê²½ì œ'
]
# ìˆ˜ì§‘í•  ë‰´ìŠ¤ì˜ ëŒ€ë¶„ë¥˜ì™€ ì†Œë¶„ë¥˜ ëª©ë¡ì„ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì •ì˜
categories = {
    'ì •ì¹˜': ['ëŒ€í†µë ¹ì‹¤', 'êµ­íšŒ', 'ì •ë‹¹', 'í–‰ì •', 'ì™¸êµ', 'êµ­ë°©/ë¶í•œ'],
    'ê²½ì œ': ['ê¸ˆìœµ/ì¦ê¶Œ', 'ì‚°ì—…/ì¬ê³„', 'ì¤‘ê¸°/ë²¤ì²˜', 'ë¶€ë™ì‚°', 'ê¸€ë¡œë²Œ', 'ìƒí™œ'],
    'ì‚¬íšŒ': ['ì‚¬ê±´ì‚¬ê³ ', 'êµìœ¡', 'ë…¸ë™', 'ì–¸ë¡ ', 'í™˜ê²½', 'ì¸ê¶Œ/ë³µì§€', 'ì‹í’ˆ/ì˜ë£Œ', 'ì§€ì—­', 'ì¸ë¬¼'],
    'IT_ê³¼í•™': ['ëª¨ë°”ì¼', 'ì¸í„°ë„·/SNS', 'í†µì‹ /ë‰´ë¯¸ë””ì–´', 'IT', 'ë³´ì•ˆ/í•´í‚¹', 'ì»´í“¨í„°', 'ê²Œì„/ë¦¬ë·°', 'ê³¼í•™'],
    'ìƒí™œ_ë¬¸í™”': ['ê±´ê°•', 'ìë™ì°¨', 'ì—¬í–‰/ë ˆì €', 'ìŒì‹/ë§›ì§‘', 'íŒ¨ì…˜/ë·°í‹°', 'ê³µì—°/ì „ì‹œ', 'ì±…', 'ì¢…êµ', 'ë‚ ì”¨', 'ìƒí™œ'],
    'ì„¸ê³„': ['ì•„ì‹œì•„/í˜¸ì£¼', 'ë¯¸êµ­/ì¤‘ë‚¨ë¯¸', 'ìœ ëŸ½', 'ì¤‘ë™/ì•„í”„ë¦¬ì¹´', 'ì„¸ê³„'],
    'ì—¬í–‰': ['êµ­ë‚´ ì—¬í–‰']
}
MAX_ARTICLES_PER_CATEGORY = 100  # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ìˆ˜ì§‘í•  ìµœëŒ€ ê¸°ì‚¬ ìˆ˜
save_path = 'C:/Users/admin/Desktop/news/test1/output'  # ê²°ê³¼ HTML íŒŒì¼ì´ ì €ì¥ë  ê²½ë¡œ
one_month_ago = datetime.now() - timedelta(days=30)  # í•œ ë‹¬ ì „ ë‚ ì§œë¥¼ ê³„ì‚° (ì´ë³´ë‹¤ ì˜¤ë˜ëœ ë‰´ìŠ¤ëŠ” ìˆ˜ì§‘ ì•ˆ í•¨)
os.makedirs(save_path, exist_ok=True)  # ì €ì¥ ê²½ë¡œì— í´ë”ê°€ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ìƒì„±

# ================================
# 5. ì›ë¬¸/ì´ë¯¸ì§€ ì£¼ì†Œ ì¶”ì¶œ í•¨ìˆ˜ (WebDriverWait ì ìš©)
# ================================
# êµ¬ê¸€ ë‰´ìŠ¤ ë§í¬ë¥¼ ì…ë ¥ë°›ì•„ ì‹¤ì œ ì›ë¬¸ ê¸°ì‚¬ ì£¼ì†Œì™€ ëŒ€í‘œ ì´ë¯¸ì§€ ì£¼ì†Œë¥¼ ì°¾ì•„ë‚´ëŠ” í•¨ìˆ˜
def get_original_article_info(google_news_url):
    print(f"  -> [ë³€í™˜ ì‹œë„] ê¸°ì¡´ ì£¼ì†Œ: {google_news_url}")  # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ êµ¬ê¸€ ë‰´ìŠ¤ URL ì¶œë ¥
    chrome_options = Options()  # í¬ë¡¬ ë¸Œë¼ìš°ì € ì˜µì…˜ ì„¤ì • ê°ì²´ ìƒì„±
    chrome_options.add_argument("--headless")  # ë¸Œë¼ìš°ì € ì°½ì„ ì‹¤ì œë¡œ ë„ìš°ì§€ ì•ŠëŠ” í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œë¡œ ì‹¤í–‰
    chrome_options.add_argument("--disable-gpu")  # GPU ê°€ì† ë¹„í™œì„±í™” (í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œì—ì„œ ì•ˆì •ì„± í–¥ìƒ)
    chrome_options.add_argument("user-agent=...")  # User-Agent ê°’ì„ ì„¤ì •í•˜ì—¬ ë´‡ìœ¼ë¡œ ì¸ì‹ë˜ëŠ” ê²ƒì„ ë°©ì§€
    service = Service()  # í¬ë¡¬ ë“œë¼ì´ë²„ ì„œë¹„ìŠ¤ ê°ì²´ ìƒì„±
    driver = webdriver.Chrome(service=service, options=chrome_options)  # ì„¤ì •ëœ ì˜µì…˜ìœ¼ë¡œ í¬ë¡¬ ë“œë¼ì´ë²„ ì‹¤í–‰
    original_url, image_url = google_news_url, None  # ì´ˆê¸°ê°’ ì„¤ì •
    try:  # ì˜¤ë¥˜ ë°œìƒ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ì½”ë“œë¥¼ try ë¸”ë¡ ì•ˆì— ì‘ì„±
        driver.get(google_news_url)  # ì…€ë ˆë‹ˆì›€ìœ¼ë¡œ êµ¬ê¸€ ë‰´ìŠ¤ URL ì ‘ì†
        # í˜ì´ì§€ê°€ ë¦¬ë””ë ‰ì…˜ë˜ì–´ í˜„ì¬ URLì— "news.google.com"ì´ ì—†ì„ ë•Œê¹Œì§€ ìµœëŒ€ 15ì´ˆê°„ ê¸°ë‹¤ë¦¼
        WebDriverWait(driver, 15).until(lambda d: "news.google.com" not in d.current_url)
        original_url = driver.current_url  # ë¦¬ë””ë ‰ì…˜ì´ ì™„ë£Œëœ ìµœì¢… URLì„ ì €ì¥
        print(f"  -> [ë³€í™˜ ì™„ë£Œ] ì›ë¬¸ ì£¼ì†Œ: {original_url}")  # ë³€í™˜ëœ ì›ë¬¸ ì£¼ì†Œ ì¶œë ¥
        soup = BeautifulSoup(driver.page_source, 'html.parser')  # í˜ì´ì§€ì˜ HTML ì†ŒìŠ¤ë¥¼ BeautifulSoupìœ¼ë¡œ íŒŒì‹±
        # HTMLì˜ <meta> íƒœê·¸ ì¤‘ property ì†ì„±ì´ 'og:image'ì¸ ê²ƒì„ ì°¾ìŒ (ëŒ€í‘œ ì´ë¯¸ì§€ë¥¼ ë‚˜íƒ€ëƒ„)
        image_tag = soup.find('meta', property='og:image')
        if image_tag and image_tag.get('content'):  # ì´ë¯¸ì§€ íƒœê·¸ì™€ ê·¸ ë‚´ìš©ì´ ì¡´ì¬í•˜ë©´
            image_url = image_tag['content']  # content ì†ì„±ê°’(ì´ë¯¸ì§€ URL)ì„ ì €ì¥
            print("  -> ì´ë¯¸ì§€ ì£¼ì†Œ ì°¾ìŒ!")
        else:  # ì´ë¯¸ì§€ íƒœê·¸ê°€ ì—†ìœ¼ë©´
            print("  [ì•Œë¦¼] ì´ í˜ì´ì§€ì—ëŠ” og:image íƒœê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:  # try ë¸”ë¡ì—ì„œ ì˜¤ë¥˜ ë°œìƒ ì‹œ
        print(f"  [ì˜¤ë¥˜] ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ (íƒ€ì„ì•„ì›ƒ ë˜ëŠ” ê¸°íƒ€): {e}")  # ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶œë ¥
        original_url = driver.current_url  # ì˜¤ë¥˜ ë°œìƒ ì‹œì ì˜ URLì„ ì €ì¥
        print(f"  -> [ì˜¤ë¥˜ ì‹œì  ì£¼ì†Œ] {original_url}")
    finally:  # ì˜¤ë¥˜ ë°œìƒ ì—¬ë¶€ì™€ ìƒê´€ì—†ì´ í•­ìƒ ì‹¤í–‰
        driver.quit()  # í¬ë¡¬ ë“œë¼ì´ë²„ ì¢…ë£Œ
    return {'original_url': original_url, 'image_url': image_url}  # ì›ë¬¸ ì£¼ì†Œì™€ ì´ë¯¸ì§€ ì£¼ì†Œë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜

# ================================
# 5-1. ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ í•¨ìˆ˜
# ================================
# ê¸°ì‚¬ URLì„ ì…ë ¥ë°›ì•„ ì›¹í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ(ìŠ¤í¬ë˜í•‘)í•˜ëŠ” í•¨ìˆ˜
def scrape_article_body(url):
    try:  # ì˜¤ë¥˜ ë°œìƒ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ì½”ë“œë¥¼ try ë¸”ë¡ ì•ˆì— ì‘ì„±
        headers = {'User-Agent': '...'}  # ë´‡ìœ¼ë¡œ ì¸ì‹ë˜ì§€ ì•Šë„ë¡ User-Agent ì„¤ì •
        # requests ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ URLì— ì ‘ì†í•˜ì—¬ í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜´ (íƒ€ì„ì•„ì›ƒ 10ì´ˆ)
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()  # HTTP ìš”ì²­ì´ ì‹¤íŒ¨í•˜ë©´(200ë²ˆëŒ€ ì½”ë“œê°€ ì•„ë‹ˆë©´) ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œí‚´
        soup = BeautifulSoup(response.text, 'html.parser')  # í˜ì´ì§€ ë‚´ìš©ì„ BeautifulSoupìœ¼ë¡œ íŒŒì‹±
        article_tag = soup.find('article')  # HTMLì˜ <article> íƒœê·¸ë¥¼ ì°¾ìŒ (ë³´í†µ ê¸°ì‚¬ ë³¸ë¬¸ì„ ê°ì‹¸ê³  ìˆìŒ)
        if not article_tag:  # <article> íƒœê·¸ë¥¼ ì°¾ì§€ ëª»í–ˆë‹¤ë©´
            # ë‹¤ë¥¸ ì–¸ë¡ ì‚¬ì—ì„œ í”íˆ ì‚¬ìš©í•˜ëŠ” idë‚˜ class ì´ë¦„ìœ¼ë¡œ ë‹¤ì‹œ ê²€ìƒ‰
            selectors = ['#articleBodyContents', '#article_body', '.article_body', '#dic_area']
            for selector in selectors:
                article_tag = soup.select_one(selector)
                if article_tag: break  # ì°¾ìœ¼ë©´ ë°˜ë³µ ì¤‘ë‹¨
        if article_tag:  # ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ íƒœê·¸ë¥¼ ì°¾ì•˜ë‹¤ë©´
            paragraphs = article_tag.find_all('p')  # ê·¸ ì•ˆì—ì„œ ëª¨ë“  <p>(ë¬¸ë‹¨) íƒœê·¸ë¥¼ ì°¾ìŒ
            if not paragraphs: paragraphs = article_tag.find_all('div')  # <p>ê°€ ì—†ìœ¼ë©´ <div> íƒœê·¸ë¥¼ ëŒ€ì‹  ì°¾ìŒ
            # ëª¨ë“  ë¬¸ë‹¨ì˜ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹¨ (ì•ë’¤ ê³µë°± ì œê±°, ë¹ˆ ë¬¸ë‹¨ ì œì™¸)
            body_text = "\n".join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
            if len(body_text) > 50:  # ë³¸ë¬¸ ê¸¸ì´ê°€ 50ì ì´ìƒì´ë©´
                print("  -> ë³¸ë¬¸ ìˆ˜ì§‘ ì„±ê³µ!")
                return body_text  # ìˆ˜ì§‘í•œ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜
        print("  [ì•Œë¦¼] ê¸°ì‚¬ ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.")
        return None  # ë³¸ë¬¸ ìˆ˜ì§‘ì— ì‹¤íŒ¨í•˜ë©´ Noneì„ ë°˜í™˜
    except Exception as e:  # ì˜¤ë¥˜ ë°œìƒ ì‹œ
        print(f"  [ì˜¤ë¥˜] ë³¸ë¬¸ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None  # ì˜¤ë¥˜ ë°œìƒ ì‹œ Noneì„ ë°˜í™˜

# ================================
# 6. ë‰´ìŠ¤ ìˆ˜ì§‘ ë° GPT í‰ê°€ í•¨ìˆ˜
# ================================
# íŠ¹ì • ì†Œë¶„ë¥˜ í‚¤ì›Œë“œë¡œ êµ¬ê¸€ ë‰´ìŠ¤ RSSì—ì„œ ê¸°ì‚¬ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def fetch_news(sub_category):
    encoded_keyword = quote(sub_category)  # í•œê¸€ í‚¤ì›Œë“œë¥¼ URLì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì¸ì½”ë”©
    news_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl=ko&gl=KR"  # êµ¬ê¸€ ë‰´ìŠ¤ RSS ì£¼ì†Œ ìƒì„±
    feed = feedparser.parse(news_url)  # feedparserë¡œ RSS í”¼ë“œë¥¼ ë¶„ì„
    articles = []  # ìˆ˜ì§‘í•œ ê¸°ì‚¬ ì •ë³´ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    for entry in feed.entries:  # RSS í”¼ë“œì— ìˆëŠ” ê° ê¸°ì‚¬(entry)ì— ëŒ€í•´ ë°˜ë³µ
        if len(articles) >= MAX_ARTICLES_PER_CATEGORY: break  # ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜ì— ë„ë‹¬í•˜ë©´ ì¤‘ë‹¨
        source = getattr(entry, 'source', None)  # ê¸°ì‚¬ì˜ ì¶œì²˜(ì–¸ë¡ ì‚¬) ì •ë³´ë¥¼ ê°€ì ¸ì˜´
        if source and source.title in all_sources:  # ì¶œì²˜ê°€ ìˆê³ , ìš°ë¦¬ê°€ ì •í•œ ì–¸ë¡ ì‚¬ ëª©ë¡ì— í¬í•¨ë˜ì–´ ìˆë‹¤ë©´
            published_time = entry.get('published_parsed')  # ê¸°ì‚¬ ë°œí–‰ ì‹œê°„ì„ ê°€ì ¸ì˜´
            if not published_time: continue  # ë°œí–‰ ì‹œê°„ì´ ì—†ìœ¼ë©´ ê±´ë„ˆëœ€
            article_date = datetime.fromtimestamp(time.mktime(published_time))  # ë°œí–‰ ì‹œê°„ì„ datetime ê°ì²´ë¡œ ë³€í™˜
            if article_date < one_month_ago: continue  # ë„ˆë¬´ ì˜¤ë˜ëœ ê¸°ì‚¬(í•œ ë‹¬ ì´ì „)ëŠ” ê±´ë„ˆëœ€
            print(f"Processing '{entry.title[:30]}...'")  # ì²˜ë¦¬ ì¤‘ì¸ ê¸°ì‚¬ ì œëª© ì¶œë ¥
            article_info = get_original_article_info(entry.link)  # ì›ë¬¸ ì£¼ì†Œì™€ ì´ë¯¸ì§€ ì£¼ì†Œ ì¶”ì¶œ
            body_text = None  # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
            if "news.google.com" not in article_info['original_url']:  # ì›ë¬¸ ì£¼ì†Œ ë³€í™˜ì— ì„±ê³µí–ˆë‹¤ë©´
                body_text = scrape_article_body(article_info['original_url'])  # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
            articles.append({  # ìˆ˜ì§‘í•œ ê¸°ì‚¬ ì •ë³´ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                'title': entry.title,
                'link': article_info['original_url'],
                'image_url': article_info['image_url'],
                'source': source.title,
                'date': article_date.strftime('%Y-%m-%d %H:%M:%S'),
                'content': body_text if body_text else entry.title  # ë³¸ë¬¸ ìˆ˜ì§‘ ì„±ê³µ ì‹œ ë³¸ë¬¸ì„, ì‹¤íŒ¨ ì‹œ ì œëª©ì„ contentë¡œ ì‚¬ìš©
            })
    return articles  # ê¸°ì‚¬ ì •ë³´ê°€ ë‹´ê¸´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜

# ê¸°ì‚¬ ë‚´ìš©(í…ìŠ¤íŠ¸)ì„ ë°›ì•„ GPT ëª¨ë¸ì— ìš”ì•½ ë° ì‹ ë¢°ë„ í‰ê°€ë¥¼ ìš”ì²­í•˜ëŠ” í•¨ìˆ˜
def gpt_evaluate(article_text, selected_sources):
    # GPTì— ë³´ë‚¼ í”„ë¡¬í”„íŠ¸(ëª…ë ¹ì–´)ë¥¼ ìƒì„±. ì—­í• , ìš”êµ¬ì‚¬í•­, í˜•ì‹ ë“±ì„ ìì„¸íˆ ì§€ì •
    prompt_text = f"..."
    # GPTì— ì „ë‹¬í•  ë©”ì‹œì§€ ëª©ë¡ì„ ìƒì„±. ì‹œìŠ¤í…œ ì—­í• , ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸, ì‹¤ì œ ê¸°ì‚¬ ë‚´ìš©ìœ¼ë¡œ êµ¬ì„±
    messages = [{"role": "system", "content": "..."}, {"role": "user", "content": prompt_text}, {"role": "user", "content": article_text}]
    try:  # ì˜¤ë¥˜ ë°œìƒ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ì½”ë“œë¥¼ try ë¸”ë¡ ì•ˆì— ì‘ì„±
        # ì„¤ì •ëœ í´ë¼ì´ì–¸íŠ¸ë¥¼ í†µí•´ Azure OpenAIì— ì±„íŒ… ì™„ë£Œ(chat completions) ìš”ì²­ì„ ë³´ëƒ„
        completion = client.chat.completions.create(model=deployment, messages=messages, max_completion_tokens=1024)
        # AIê°€ ìƒì„±í•œ ë‹µë³€ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜ (ì•ë’¤ ê³µë°± ì œê±°)
        return completion.choices[0].message.content.strip()
    except Exception as e:  # ì˜¤ë¥˜ ë°œìƒ ì‹œ
        return f"GPT í‰ê°€ ì˜¤ë¥˜: {e}"  # ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ í¬í•¨í•œ ë¬¸ìì—´ì„ ë°˜í™˜

# ================================
# 7. Azure ë²ˆì—­ í•¨ìˆ˜
# ================================
# í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ì§€ì •ëœ ì—¬ëŸ¬ ì–¸ì–´ë¡œ ë²ˆì—­í•˜ëŠ” í•¨ìˆ˜
def translate_with_azure(text_to_translate, target_languages):
    headers = {'Ocp-Apim-Subscription-Key': translator_key, ...}  # ìš”ì²­ í—¤ë”ì— ì¸ì¦ í‚¤ ë“±ì„ í¬í•¨
    params = {'api-version': '3.0', 'from': 'ko', 'to': target_languages}  # ìš”ì²­ íŒŒë¼ë¯¸í„°ì— API ë²„ì „, ì¶œë°œ/ë„ì°© ì–¸ì–´ ë“±ì„ ì„¤ì •
    body = [{'text': text_to_translate}]  # ìš”ì²­ ë³¸ë¬¸ì— ë²ˆì—­í•  í…ìŠ¤íŠ¸ë¥¼ í¬í•¨
    try:  # ì˜¤ë¥˜ ë°œìƒ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ì½”ë“œë¥¼ try ë¸”ë¡ ì•ˆì— ì‘ì„±
        # requests ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ Azure ë²ˆì—­ APIì— POST ìš”ì²­ì„ ë³´ëƒ„
        response = requests.post(f"{translator_endpoint}/translate", params=params, headers=headers, json=body)
        response.raise_for_status()  # HTTP ìš”ì²­ ì‹¤íŒ¨ ì‹œ ì˜¤ë¥˜ ë°œìƒ
        translations = response.json()  # ì‘ë‹µë°›ì€ JSON ë°ì´í„°ë¥¼ íŒŒì‹±
        # ë²ˆì—­ ê²°ê³¼ë¥¼ {ì–¸ì–´ì½”ë“œ: ë²ˆì—­ëœ í…ìŠ¤íŠ¸} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ë¡œ ë§Œë“¤ì–´ ë°˜í™˜
        return {t['to']: t['text'] for t in translations[0]['translations']}
    except Exception as e:  # ì˜¤ë¥˜ ë°œìƒ ì‹œ
        print(f"Azure ë²ˆì—­ API ì˜¤ë¥˜: {e}")
        return {lang: "ë²ˆì—­ ì˜¤ë¥˜" for lang in target_languages}  # ê° ì–¸ì–´ì— ëŒ€í•´ "ë²ˆì—­ ì˜¤ë¥˜" ë©”ì‹œì§€ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜

# ================================
# 8. HTML ì €ì¥ í•¨ìˆ˜
# ================================
# ìˆ˜ì§‘í•˜ê³  ì²˜ë¦¬í•œ ê¸°ì‚¬ ëª©ë¡ì„ ë°›ì•„ í•˜ë‚˜ì˜ HTML íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def save_news_with_translations(main_category, sub_category, articles):
    main_path = os.path.join(save_path, main_category)  # ëŒ€ë¶„ë¥˜ í´ë” ê²½ë¡œ ìƒì„±
    os.makedirs(main_path, exist_ok=True)  # í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
    file_name = f"{sub_category.replace('/', '_')}_news.html"  # íŒŒì¼ ì´ë¦„ ì„¤ì • (ìŠ¬ë˜ì‹œ ë¬¸ìëŠ” ì–¸ë”ë°”ë¡œ ë³€ê²½)
    full_path = os.path.join(main_path, file_name)  # ì „ì²´ íŒŒì¼ ê²½ë¡œ ìƒì„±
    with open(full_path, 'w', encoding='utf-8') as f:  # íŒŒì¼ì„ ì“°ê¸° ëª¨ë“œ('w')ì™€ UTF-8 ì¸ì½”ë”©ìœ¼ë¡œ ì—´ê¸°
        # HTMLì˜ ê¸°ë³¸ êµ¬ì¡°ì™€ CSS ìŠ¤íƒ€ì¼ ì‹œíŠ¸ ì‘ì„±
        f.write("<html><head>...</head><body>")
        f.write(f"<h1>{main_category} - {sub_category} ë‰´ìŠ¤</h1>")  # í˜ì´ì§€ ì œëª© ì‘ì„±

        # ì „ì²´ ì–¸ì–´ë¥¼ í•œ ë²ˆì— ë°”ê¿€ ìˆ˜ ìˆëŠ” ë²„íŠ¼ë“¤ ìƒì„±
        f.write('<div class="lang-buttons">...</div><hr>')

        for i, article in enumerate(articles):  # ê° ê¸°ì‚¬ì— ëŒ€í•´ ë°˜ë³µ
            f.write(f'<div class="article-block" id="article-{i}">')  # ê¸°ì‚¬ í•˜ë‚˜ë¥¼ ê°ì‹¸ëŠ” div ë¸”ë¡ ìƒì„±
            if article.get('image_url'):  # ì´ë¯¸ì§€ URLì´ ìˆë‹¤ë©´
                f.write(f"<img src='{article['image_url']}' ...>")  # <img> íƒœê·¸ë¡œ ì´ë¯¸ì§€ í‘œì‹œ
            else:  # ì´ë¯¸ì§€ URLì´ ì—†ë‹¤ë©´
                f.write("<span class='no-image-text'>[ì´ë¯¸ì§€ ì—†ìŒ]</span>")  # í…ìŠ¤íŠ¸ë¡œ í‘œì‹œ
            
            f.write(f"<p><b>ì–¸ë¡ ì‚¬:</b> {article['source']} | <b>ë°œí–‰ ì‹œê°„:</b> {article['date']}</p>")  # ì–¸ë¡ ì‚¬ì™€ ë°œí–‰ ì‹œê°„ í‘œì‹œ
            f.write('<div class="content-wrapper">')  # ì œëª©ê³¼ ìš”ì•½ì„ ê°ì‹¸ëŠ” div ìƒì„±
            for lang, content in article['translations'].items():  # ê° ì–¸ì–´ë³„ ë²ˆì—­ ë‚´ìš©ì— ëŒ€í•´ ë°˜ë³µ
                active_class = "active" if lang == 'ko' else ""  # í•œêµ­ì–´ ì½˜í…ì¸ ëŠ” ê¸°ë³¸ìœ¼ë¡œ ë³´ì´ë„ë¡ 'active' í´ë˜ìŠ¤ ì¶”ê°€
                # ... ì–¸ì–´ë³„ ì œëª©ê³¼ ìš”ì•½ ë‚´ìš©ì„ HTML êµ¬ì¡°ì— ë§ê²Œ ì‘ì„± ...
            f.write('</div></div>')

        # ì–¸ì–´ ë³€ê²½ ë²„íŠ¼ì´ ì‘ë™í•˜ë„ë¡ í•˜ëŠ” ìë°”ìŠ¤í¬ë¦½íŠ¸ ì½”ë“œ ì‘ì„±
        f.write('<script>function changeAllLanguages(lang){...}</script>')
        f.write("</body></html>")  # HTML íŒŒì¼ ë‹«ê¸°

# ================================
# 9. ë©”ì¸ ì‹¤í–‰ ë£¨í”„
# ================================
target_languages = ['en', 'ja', 'fr', 'zh-Hans']  # ë²ˆì—­í•  ëª©í‘œ ì–¸ì–´ ëª©ë¡

# `categories` ë”•ì…”ë„ˆë¦¬ì— ì •ì˜ëœ ëª¨ë“  ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ ë°˜ë³µ ì‹¤í–‰
for main_category, sub_categories in categories.items():
    if main_category not in user_follow_categories: continue  # ì‚¬ìš©ìê°€ ì„ íƒí•œ ì¹´í…Œê³ ë¦¬ê°€ ì•„ë‹ˆë©´ ê±´ë„ˆëœ€
    for sub_category in sub_categories:  # ëŒ€ë¶„ë¥˜ ì•ˆì˜ ì†Œë¶„ë¥˜ì— ëŒ€í•´ ë°˜ë³µ
        print(f"[{main_category}] {sub_category} ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")  # í˜„ì¬ ìˆ˜ì§‘ ì¤‘ì¸ ì¹´í…Œê³ ë¦¬ ì¶œë ¥
        articles = fetch_news(sub_category)  # í•´ë‹¹ ì†Œë¶„ë¥˜ì˜ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ìˆ˜ì§‘
        if not articles:  # ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´
            print("  -> ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            continue  # ë‹¤ìŒ ì†Œë¶„ë¥˜ë¡œ ë„˜ì–´ê°
        processed_articles = []  # ì²˜ë¦¬ëœ ê¸°ì‚¬ ì •ë³´ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        for article in articles:  # ìˆ˜ì§‘ëœ ê° ê¸°ì‚¬ì— ëŒ€í•´ ë°˜ë³µ
            print(f"  - '{article['title'][:30]}...' GPT í‰ê°€ ë° ë²ˆì—­ ì¤‘...")  # ì²˜ë¦¬ ì¤‘ì¸ ê¸°ì‚¬ ì œëª© ì¶œë ¥

            # ê¸°ì‚¬ ë‚´ìš©(content)ì„ GPTì— ë³´ë‚´ ìš”ì•½ ë° í‰ê°€ë¥¼ ë°›ìŒ
            evaluation_text = gpt_evaluate(article['content'], user_selected_sources)

            # [ë””ë²„ê¹… ì½”ë“œ] AIì˜ ì‹¤ì œ ì‘ë‹µì„ í„°ë¯¸ë„ì— ê·¸ëŒ€ë¡œ ì¶œë ¥
            print("---------- GPT Raw Response ----------")
            print(evaluation_text)
            print("------------------------------------")

            time.sleep(1)  # APIì— ë„ˆë¬´ ë§ì€ ìš”ì²­ì„ ì§§ì€ ì‹œê°„ì— ë³´ë‚´ì§€ ì•Šë„ë¡ 1ì´ˆ ëŒ€ê¸°
            # ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•´ GPT ì‘ë‹µì—ì„œ '1)'ë¡œ ì‹œì‘í•˜ëŠ” ìš”ì•½ ë¶€ë¶„ì„ ì¶”ì¶œ
            summary_match = re.search(r'1\)(.*?)(?=2\)|\Z)', evaluation_text, re.DOTALL)
            # ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•´ GPT ì‘ë‹µì—ì„œ 'ì‹ ë¢°ë„:' ë¶€ë¶„ì„ ì¶”ì¶œ
            reliability_match = re.search(r'ì‹ ë¢°ë„:\s*(ë†’ìŒ|ë³´í†µ|ë‚®ìŒ)', evaluation_text)
            # ìš”ì•½ ì¶”ì¶œì— ì„±ê³µí•˜ë©´ ê·¸ ë‚´ìš©ì„, ì‹¤íŒ¨í•˜ë©´ "ìš”ì•½ ì •ë³´ ì—†ìŒ"ì„ ì €ì¥
            summary_text = summary_match.group(1).strip() if summary_match else "ìš”ì•½ ì •ë³´ ì—†ìŒ"
            # ì‹ ë¢°ë„ ì¶”ì¶œì— ì„±ê³µí•˜ë©´ ê·¸ ë‚´ìš©ì„, ì‹¤íŒ¨í•˜ë©´ "ì•Œ ìˆ˜ ì—†ìŒ"ì„ ì €ì¥
            reliability = reliability_match.group(1).strip() if reliability_match else "ì•Œ ìˆ˜ ì—†ìŒ"
            # ê¸°ì‚¬ ì œëª©ê³¼ ìš”ì•½ë¬¸ì„ Azure ë²ˆì—­ ì„œë¹„ìŠ¤ì— ë³´ë‚´ ë²ˆì—­
            azure_translations = translate_with_azure(f"{article['title']}\n{summary_text}", target_languages)
            # ë²ˆì—­ëœ ê²°ê³¼ì—ì„œ ì œëª© ë¶€ë¶„ë§Œ ì¶”ì¶œ
            translated_titles = {lang: trans_text.split('\n', 1)[0] for lang, trans_text in azure_translations.items()}
            # ì‹ ë¢°ë„ ë“±ê¸‰ì— ë”°ë¼ HTMLì—ì„œ ì‚¬ìš©í•  CSS í´ë˜ìŠ¤ ì´ë¦„ì„ ê²°ì •
            reliability_class = {"ë†’ìŒ": "high", "ë³´í†µ": "medium"}.get(reliability, "low")
            # ìš”ì•½ê³¼ ì‹ ë¢°ë„ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ” HTML ì¡°ê°ì„ ìƒì„±
            summary_html = f"<div class='summary'>{summary_text.replace('\n', '<br>')}<span class='reliability {reliability_class}'>ì‹ ë¢°ë„: {reliability}</span></div>"
            # ìµœì¢…ì ìœ¼ë¡œ HTML íŒŒì¼ì— ì €ì¥í•  ëª¨ë“  ê¸°ì‚¬ ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì •ë¦¬
            article_data = {
                'link': article['link'], 'image_url': article['image_url'], 'source': article['source'], 'date': article['date'],
                'translations': {
                    'ko': {'title': article['title'], 'summary_html': summary_html},
                    'en': translated_titles.get('en', "ë²ˆì—­ ì˜¤ë¥˜"), 'ja': translated_titles.get('ja', "ë²ˆì—­ ì˜¤ë¥˜"),
                    'fr': translated_titles.get('fr', "ë²ˆì—­ ì˜¤ë¥˜"), 'zh-Hans': translated_titles.get('zh-Hans', "ë²ˆì—­ ì˜¤ë¥˜")
                }
            }
            processed_articles.append(article_data)  # ì²˜ë¦¬ëœ ê¸°ì‚¬ ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        # í•´ë‹¹ ì†Œë¶„ë¥˜ì˜ ëª¨ë“  ê¸°ì‚¬ ì²˜ë¦¬ê°€ ëë‚˜ë©´ HTML íŒŒì¼ë¡œ ì €ì¥
        save_news_with_translations(main_category, sub_category, processed_articles)
        print(f"  -> {len(processed_articles)}ê°œ ë‰´ìŠ¤ ì €ì¥ ì™„ë£Œ.")
print("\nğŸ‰ ëª¨ë“  ë‰´ìŠ¤ ìˆ˜ì§‘, í‰ê°€ ë° ë²ˆì—­ ì™„ë£Œ!")  # ëª¨ë“  ì‘ì—…ì´ ëë‚˜ë©´ ì™„ë£Œ ë©”ì‹œì§€ ì¶œë ¥
