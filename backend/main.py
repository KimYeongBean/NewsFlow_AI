# ================================
# 0. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ================================
import asyncio
import feedparser
import time
import os
from datetime import datetime, timedelta
from urllib.parse import quote, urljoin, urlparse, parse_qs
from openai import AzureOpenAI
import re
import json
import requests
import uuid
from bs4 import BeautifulSoup
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Dict, Optional
from dotenv import load_dotenv
from azure.cosmos import CosmosClient, PartitionKey, exceptions

# -------------------------
# Publisher mapping (simple whitelist)
# -------------------------
PUBLISHER_MAP = {
    'íŒŒì´ë‚¸ì…œë‰´ìŠ¤': 'fnnews.com',
    'ì¡°ì„ ì¼ë³´': 'chosun.com',
    'ì—°í•©ë‰´ìŠ¤': 'yna.co.kr',
    'ì¡°ì„ ': 'chosun.com',
    'í•œê²¨ë ˆ': 'hani.co.kr',
}

# ================================
# 1. ì´ˆê¸° ì„¤ì • ë° í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# ================================
load_dotenv() # .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ

# --- FastAPI ì•± ìƒì„± ë° CORS ì„¤ì • ---
app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œëŠ” í”„ë¡ íŠ¸ì—”ë“œ URLë§Œ í—ˆìš©í•˜ë„ë¡ ë³€ê²½
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- ddd ë²„ì „ì˜ ì„¤ì •ê°’ ì¶”ê°€ ---
user_selected_sources = ["ì¡°ì„ ì¼ë³´", "í•œê²¨ë ˆ", "ì¤‘ì•™ì¼ë³´", "ë™ì•„ì¼ë³´", "ê²½í–¥ì‹ ë¬¸"]
user_follow_categories = ["ì •ì¹˜", "ê²½ì œ", "IT_ê³¼í•™"] # ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•  ì¹´í…Œê³ ë¦¬
# í…ŒìŠ¤íŠ¸ìš©: ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” ì•„ë˜ ë¼ì¸ìœ¼ë¡œ ë®ì–´ì¨ì„œ í•œ ê°œ ì¹´í…Œê³ ë¦¬ì™€ ì†Œë¶„ë¥˜ë§Œ ìˆ˜ì§‘í•˜ë„ë¡ ì œí•œí•©ë‹ˆë‹¤.
# ì‹¤ì œ ìš´ì˜ ì‹œì—ëŠ” ì´ ì£¼ì„ì„ ì§€ìš°ê³  ìœ„ ì„¤ì •ì„ ì‚¬ìš©í•˜ì„¸ìš”.
# user_follow_categories = ["ì •ì¹˜"]
all_sources = [
    'MBCë‰´ìŠ¤', 'ì—°í•©ë‰´ìŠ¤', 'ì¡°ì„ ì¼ë³´', 'ë‰´ìŠ¤1', 'JTBC ë‰´ìŠ¤',
    'ì¤‘ì•™ì¼ë³´', 'SBS ë‰´ìŠ¤', 'YTN', 'í•œê²¨ë ˆ', 'ê²½í–¥ì‹ ë¬¸',
    'ì˜¤ë§ˆì´ë‰´ìŠ¤', 'í•œêµ­ê²½ì œ'
]
categories = {
    'ì •ì¹˜': ['ëŒ€í†µë ¹ì‹¤', 'êµ­íšŒ', 'ì •ë‹¹', 'í–‰ì •', 'ì™¸êµ', 'êµ­ë°©/ë¶í•œ'],
    'ê²½ì œ': ['ê¸ˆìœµ/ì¦ê¶Œ', 'ì‚°ì—…/ì¬ê³„', 'ì¤‘ê¸°/ë²¤ì²˜', 'ë¶€ë™ì‚°', 'ê¸€ë¡œë²Œ', 'ìƒí™œ'],
    'ì‚¬íšŒ': ['ì‚¬ê±´ì‚¬ê³ ', 'êµìœ¡', 'ë…¸ë™', 'ì–¸ë¡ ', 'í™˜ê²½', 'ì¸ê¶Œ/ë³µì§€', 'ì‹í’ˆ/ì˜ë£Œ', 'ì§€ì—­', 'ì¸ë¬¼'],
    'IT_ê³¼í•™': ['ëª¨ë°”ì¼', 'ì¸í„°ë„·/SNS', 'í†µì‹ /ë‰´ë¯¸ë””ì–´', 'IT', 'ë³´ì•ˆ/í•´í‚¹', 'ì»´í“¨í„°', 'ê²Œì„/ë¦¬ë·°', 'ê³¼í•™'],
    'ìƒí™œ_ë¬¸í™”': ['ê±´ê°•', 'ìë™ì°¨', 'ì—¬í–‰/ë ˆì €', 'ìŒì‹/ë§›ì§‘', 'íŒ¨ì…˜/ë·°í‹°', 'ê³µì—°/ì „ì‹œ', 'ì±…', 'ì¢…êµ', 'ë‚ ì”¨', 'ìƒí™œ'],
    'ì„¸ê³„': ['ì•„ì‹œì•„/í˜¸ì£¼', 'ë¯¸êµ­/ì¤‘ë‚¨ë¯¸', 'ìœ ëŸ½', 'ì¤‘ë™/ì•„í”„ë¦¬ì¹´', 'ì„¸ê³„'],
}

# --- Azure Cosmos DB ì„¤ì • ---
COSMOS_DB_URI = os.getenv("COSMOS_DB_URI")
COSMOS_DB_KEY = os.getenv("COSMOS_DB_KEY")
DATABASE_NAME = "NewsFlowDB"
CONTAINER_NAME = "NewsItems"

# Cosmos DB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
container = None
if COSMOS_DB_URI and COSMOS_DB_KEY:
    try:
        cosmos_client = CosmosClient(COSMOS_DB_URI, credential=COSMOS_DB_KEY)
        database = cosmos_client.create_database_if_not_exists(DATABASE_NAME)
        container = database.create_container_if_not_exists(CONTAINER_NAME, partition_key=PartitionKey(path="/category"))
        print("Azure Cosmos DBì— ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"[ì˜¤ë¥˜] Azure Cosmos DB ì—°ê²° ì‹¤íŒ¨: {e}")
else:
    print("[ê²½ê³ ] Cosmos DB í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# --- Azure AI ì„œë¹„ìŠ¤ ì„¤ì • ---
translator_key = os.getenv("AZURE_TRANSLATOR_KEY")
translator_endpoint = "https://api.cognitive.microsofttranslator.com/"
translator_location = "KoreaCentral"

# Optional Selenium fallback (disabled by default). Set USE_SELENIUM=true for local debugging if needed.
USE_SELENIUM = os.getenv('USE_SELENIUM', 'false').lower() == 'true'

# --- Azure OpenAI ì„¤ì • ---
openai_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
openai_deployment = "gpt-4o"
openai_key = os.getenv("AZURE_OPENAI_KEY")
openai_api_version = "2025-01-01-preview"

# Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
openai_client = None
if openai_endpoint and openai_key:
    try:
        openai_client = AzureOpenAI(
            azure_endpoint=openai_endpoint,
            api_key=openai_key,
            api_version=openai_api_version,
        )
        print("Azure OpenAI ì„œë¹„ìŠ¤ì— ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"[ì˜¤ë¥˜] Azure OpenAI ì—°ê²° ì‹¤íŒ¨: {e}")
else:
    print("[ê²½ê³ ] Azure OpenAI í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# ================================
# 2. Pydantic ë°ì´í„° ëª¨ë¸ ì •ì˜
# ================================
class NewsArticle(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    category: str
    subCategory: str
    link: str
    source: str
    date: str
    summary: str
    reliability: str
    translatedTitles: Dict[str, str]
    translatedSummaries: Dict[str, str]
    imageUrl: Optional[str] = None
    evaluation: str

# ================================
# 3. í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜
# ================================
def get_original_article_info(google_news_url):
    """requests ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… URLê³¼ og:imageë¥¼ ì‹œë„í•©ë‹ˆë‹¤. Selenium ì˜ì¡´ì„±ì„ ì œê±°í–ˆì§€ë§Œ
    ë™ì‘ì€ ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ìµœì¢… URLì„ ì°¾ì•„ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤. ë§Œì•½ requestsë¡œ ì›ë¬¸ì„ ì°¾ì§€ ëª»í•˜ë©´
    ì…ë ¥ëœ URLì„ ê·¸ëŒ€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print(f"  -> [ë³€í™˜ ì‹œë„] ê¸°ì¡´ ì£¼ì†Œ: {google_news_url}")
    image_url = None
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        resp = requests.get(google_news_url, headers=headers, timeout=10, allow_redirects=True)
        final_url = resp.url
        soup = BeautifulSoup(resp.text, 'html.parser')

        # Helper: try to fetch candidate page and extract og:image
        def fetch_and_get_og_image(url_candidate):
            try:
                r2 = requests.get(url_candidate, headers=headers, timeout=10, allow_redirects=True)
                s2 = BeautifulSoup(r2.text, 'html.parser')
                og = s2.find('meta', property='og:image') or s2.find('meta', attrs={'name': 'og:image'})
                if og and og.get('content'):
                    return og.get('content')
                # try common image selectors
                img = s2.find('meta', property='twitter:image') or s2.find('link', rel='image_src')
                if img and img.get('content'):
                    return img.get('content')
                if img and img.get('href'):
                    return img.get('href')
                # look for first large <img>
                imgs = s2.find_all('img')
                for im in imgs:
                    src = im.get('src') or im.get('data-src') or im.get('data-original')
                    if src and src.startswith('http') and 'logo' not in src and 'icon' not in src:
                        return src
            except Exception:
                return None
            return None

        # 1) ê¸°ë³¸ì ìœ¼ë¡œ í˜„ì¬ ì‘ë‹µ í˜ì´ì§€ì—ì„œ og:image ì¶”ì¶œ
        image_tag = soup.find('meta', property='og:image') or soup.find('meta', attrs={'name': 'og:image'})
        if image_tag and image_tag.get('content'):
            image_url = image_tag['content']

        # 2) ë§Œì•½ final_urlì´ êµ¬ê¸€ ë‰´ìŠ¤ í˜ì´ì§€ì´ê±°ë‚˜ ì´ë¯¸ì§€ëŠ” êµ¬ê¸€usercontent(í”„ë¦¬ë·°)ì¼ ê²½ìš°, ì›ë¬¸ ë§í¬ íƒìƒ‰ ë° ì›ë¬¸ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹œë„
        need_try_original = False
        if 'news.google.com' in final_url:
            need_try_original = True
        if image_url and 'lh3.googleusercontent.com' in image_url:
            need_try_original = True

        if need_try_original:
            # ìš°ì„  og:url / canonicalì„ í™•ì¸
            meta_url = None
            og_url_tag = soup.find('meta', property='og:url')
            if og_url_tag and og_url_tag.get('content'):
                meta_url = og_url_tag.get('content')
            canon = soup.find('link', rel='canonical')
            if not meta_url and canon and canon.get('href'):
                meta_url = canon.get('href')

            candidate = None
            if meta_url and 'news.google.com' not in meta_url:
                candidate = meta_url
            else:
                # try parsing anchors and url?q patterns
                anchors = soup.find_all('a', href=True)
                for a in anchors:
                    href = a['href']
                    if 'url?q=' in href or '/url?' in href:
                        parsed = urlparse(href)
                        qs = parse_qs(parsed.query)
                        for key in ('q', 'url'):
                            if key in qs and qs[key]:
                                cand = qs[key][0]
                                if cand.startswith('http') and 'news.google.com' not in cand:
                                    candidate = cand
                                    break
                        if candidate:
                            break
                    if href.startswith('http') and 'news.google.com' not in href:
                        candidate = href
                        break
                    if href.startswith('/'):
                        cand = urljoin(final_url, href)
                        if 'news.google.com' not in cand:
                            candidate = cand
                            break

                # additional heuristic: search for attributes that may contain the original URL
                if not candidate:
                    for tag in soup.find_all(True):
                        for attr in ('data-url', 'data-href', 'data-expanded-url', 'data-origin', 'data-amp-url', 'data-src'):
                            val = tag.get(attr)
                            if not val:
                                continue
                            # extract first http(s) URL from attribute
                            m = re.search(r'https?://[^"\'\s>]+', val)
                            if m:
                                cand = m.group(0)
                                if 'news.google.com' not in cand:
                                    candidate = cand
                                    break
                        if candidate:
                            break
                # last resort: search script text for embedded URLs
                if not candidate:
                    scripts = soup.find_all('script')
                    for sc in scripts:
                        txt = sc.string or ''
                        if not txt:
                            continue
                        m = re.search(r'https?://[^"\'\s>]+', txt)
                        if m:
                            cand = m.group(0)
                            if 'news.google.com' not in cand:
                                candidate = cand
                                break

            # if candidate found, try to fetch its og:image
            if candidate:
                fetched_img = fetch_and_get_og_image(candidate)
                if fetched_img:
                    image_url = fetched_img
                    final_url = candidate
                else:
                    # try amp link
                    amp = soup.find('link', rel='amphtml')
                    if amp and amp.get('href'):
                        amp_candidate = amp.get('href')
                        fetched_img = fetch_and_get_og_image(amp_candidate)
                        if fetched_img:
                            image_url = fetched_img
                            final_url = amp_candidate
        # ë§ˆì§€ë§‰ ì‹œë„: ë§Œì•½ image_url ì—†ìŒì´ë©´ entry-level media may exist; caller handles none
        if image_url:
            print("  -> ì´ë¯¸ì§€ ì£¼ì†Œ ì°¾ìŒ (requests)!")
        return {'original_url': final_url, 'image_url': image_url}
    except Exception as e:
        print(f"  [ì˜¤ë¥˜] requestsë¡œ ì›ë¬¸ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {'original_url': google_news_url, 'image_url': None}


def extract_original_from_entry(entry):
    """Try various RSS entry fields to extract the original article URL.
    Returns a URL string or None.
    """
    try:
        # 1) entry.id or guid
        eid = getattr(entry, 'id', None) or getattr(entry, 'guid', None)
        if eid and isinstance(eid, str):
            m = re.search(r"https?://[^\s\"']+", eid)
            if m:
                url = m.group(0)
                if 'news.google.com' not in url:
                    print(f"  -> [RSS:id] extracted: {url}")
                    return url

        # 2) links array (common case)
        links = getattr(entry, 'links', None)
        if links:
            for l in links:
                href = l.get('href') or l.get('link')
                rel = l.get('rel', '')
                ltype = l.get('type', '')
                if href and href.startswith('http') and 'news.google.com' not in href:
                    # prefer alternate/text/html
                    if 'alternate' in rel or 'html' in ltype or True:
                        print(f"  -> [RSS:links] candidate: {href}")
                        return href

        # 3) summary HTML anchor
        summary = getattr(entry, 'summary', None) or getattr(entry, 'summary_detail', None)
        if summary:
            val = summary if isinstance(summary, str) else (getattr(summary, 'value', '') or '')
            soup = BeautifulSoup(val, 'html.parser')
            a = soup.find('a', href=True)
            if a:
                href = a['href']
                if href.startswith('/'):
                    href = urljoin(getattr(entry, 'link', ''), href)
                if href and href.startswith('http') and 'news.google.com' not in href:
                    print(f"  -> [RSS:summary] anchor: {href}")
                    return href

        # 4) entry.link query params (e.g., /url?q=)
        link = getattr(entry, 'link', None)
        if link and isinstance(link, str):
            parsed = urlparse(link)
            qs = parse_qs(parsed.query)
            for key in ('q', 'url'):
                if key in qs and qs[key]:
                    cand = qs[key][0]
                    if cand.startswith('http') and 'news.google.com' not in cand:
                        print(f"  -> [RSS:link-q] found: {cand}")
                        return cand

        # 5) media:content / media_thumbnail fields sometimes contain original image URL only
        # not used here as original link, skip
    except Exception as e:
        print(f"  [WARN] extract_original_from_entry error: {e}")
    return None


def call_external_og_api(url):
    """í´ë°±: ì™¸ë¶€ OG ë¯¸ë¦¬ë³´ê¸° API í˜¸ì¶œ. APIí‚¤ê°€ ì—†ìœ¼ë©´ None ë°˜í™˜."""
    try:
        # Microlink ê¸°ë³¸ REST í˜¸ì¶œ: https://api.microlink.io?url={URL}
        api_url = "https://api.microlink.io/"
        params = {'url': url}
        headers = {'User-Agent': 'NewsFlowAI/1.0'}
        r = requests.get(api_url, params=params, headers=headers, timeout=10)
        r.raise_for_status()
        data = r.json()

        img = None
        # Microlink ì‘ë‹µ êµ¬ì¡°ì— ë”°ë¼ ì•ˆì „í•˜ê²Œ íŒŒì‹±
        root = data.get('data') if isinstance(data, dict) else data
        if not root:
            root = data

        if isinstance(root, dict):
            # 1) root.image.url
            image_block = root.get('image')
            if isinstance(image_block, dict):
                img = image_block.get('url') or image_block.get('src')
            # 2) root.meta.image
            if not img:
                meta = root.get('meta') or {}
                if isinstance(meta, dict):
                    img = meta.get('image') or meta.get('og:image')
            # 3) root.thumbnail or root.screenshot
            if not img:
                img = root.get('thumbnail') or root.get('screenshot')

        if img:
            print(f"[IMAGE] Microlink ì„±ê³µ -> {img}")
            return img
        else:
            print(f"[WARN] Microlink í˜¸ì¶œ ì„±ê³µí–ˆì§€ë§Œ ì´ë¯¸ì§€ ë¯¸ë°œê²¬, status: {data.get('status') if isinstance(data, dict) else 'unknown'}")
            return None
    except Exception as e:
        print(f"[WARN] Microlink í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return None


def extract_image_hybrid(article_data):
    """í•˜ì´ë¸Œë¦¬ë“œ í´ë°± ì „ëµìœ¼ë¡œ ì´ë¯¸ì§€ URL ì¶”ì¶œ.
    article_data: dict-like with keys: 'link', 'source', 'media_thumbnail', 'media_content', 'summary'
    """
    link = article_data.get('link')
    source = article_data.get('source')

    # 1ë‹¨ê³„: RSS í”¼ë“œ ë ˆë²¨(media_thumbnail/media_content)
    try:
        # media_thumbnail
        mt = article_data.get('media_thumbnail') or article_data.get('media:thumbnail')
        if mt:
            if isinstance(mt, list) and mt:
                cand = mt[0].get('url') if isinstance(mt[0], dict) else mt[0]
                if cand:
                    print(f"[IMAGE] ë‹¨ê³„1: media_thumbnail ì„±ê³µ -> {cand}")
                    return cand
            elif isinstance(mt, dict):
                cand = mt.get('url')
                if cand:
                    print(f"[IMAGE] ë‹¨ê³„1: media_thumbnail dict ì„±ê³µ -> {cand}")
                    return cand
        # media_content
        mc = article_data.get('media_content') or article_data.get('media:content')
        if mc:
            if isinstance(mc, list) and mc:
                cand = mc[0].get('url') if isinstance(mc[0], dict) else mc[0]
                if cand:
                    print(f"[IMAGE] ë‹¨ê³„1: media_content ì„±ê³µ -> {cand}")
                    return cand
            elif isinstance(mc, dict):
                cand = mc.get('url')
                if cand:
                    print(f"[IMAGE] ë‹¨ê³„1: media_content dict ì„±ê³µ -> {cand}")
                    return cand
    except Exception as e:
        print(f"[WARN] ë‹¨ê³„1 ì˜¤ë¥˜: {e}")

    # helper: parse srcset and return largest url
    def parse_srcset_get_largest(srcset):
        try:
            parts = [p.strip() for p in srcset.split(',') if p.strip()]
            best_url = None
            best_w = 0
            for p in parts:
                segs = p.split()
                url = segs[0]
                w = 0
                if len(segs) > 1 and segs[1].endswith('w'):
                    try:
                        w = int(segs[1][:-1])
                    except Exception:
                        w = 0
                if w > best_w:
                    best_w = w
                    best_url = url
            return best_url
        except Exception:
            return None

    # 2/3ë‹¨ê³„(í¼ë¸”ë¦¬ì…” ë§¤í•‘, í˜ì´ì§€ íŒŒì‹±)ë¥¼ ì œê±°í•˜ê³  Microlinkë¡œ ì§ì ‘ í´ë°±í•©ë‹ˆë‹¤.
    # MicrolinkëŠ” í˜ì´ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ë©”íƒ€/ìŠ¤í¬ë¦°ìƒ· ë“±ì˜ ì´ë¯¸ì§€ë¥¼ ì œê³µí•˜ë¯€ë¡œ
    # ì—¬ê¸°ì„œëŠ” feed ë ˆë²¨(media_*) ì´í›„ Microlink í˜¸ì¶œë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    try:
        if link:
            ext = call_external_og_api(link)
            if ext:
                print(f"[IMAGE] Microlink í´ë°± ì„±ê³µ -> {ext}")
                return ext
    except Exception as e:
        print(f"[WARN] Microlink í´ë°± ì‹¤íŒ¨: {e}")

    # 4ë‹¨ê³„: ì™¸ë¶€ OG ë¯¸ë¦¬ë³´ê¸° API í´ë°±
    try:
        if link:
            ext = call_external_og_api(link)
            if ext:
                print(f"[IMAGE] ë‹¨ê³„4: ì™¸ë¶€ OG API ì„±ê³µ -> {ext}")
                return ext
    except Exception as e:
        print(f"[WARN] ë‹¨ê³„4 ì˜¤ë¥˜: {e}")

    print("[IMAGE] ëª¨ë“  ë‹¨ê³„ ì‹¤íŒ¨: ì´ë¯¸ì§€ ì—†ìŒ")
    return None

def scrape_article_body(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        article_tag = soup.find('article')
        if not article_tag:
            selectors = ['#articleBodyContents', '#article_body', '.article_body', '#dic_area']
            for selector in selectors:
                article_tag = soup.select_one(selector)
                if article_tag: break
        if article_tag:
            body_text = "\n".join([p.get_text(strip=True) for p in article_tag.find_all(['p', 'div']) if p.get_text(strip=True)])
            if len(body_text) > 50:
                return body_text
        return None
    except Exception as e:
        print(f"  [ì˜¤ë¥˜] ë³¸ë¬¸ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def gpt_evaluate(article_text, selected_sources):
    """GPTì— ê¸°ì‚¬ í…ìŠ¤íŠ¸ë¥¼ ë³´ë‚´ ìš”ì•½(summary)ê³¼ ì‹ ë¢°ë„(reliability)ë¥¼ JSONìœ¼ë¡œ ë°˜í™˜í•˜ë„ë¡ ìš”ì²­í•©ë‹ˆë‹¤.
    ë°˜í™˜ê°’: ì„±ê³µ ì‹œ dict {"summary": str, "reliability": str, "raw": str}, ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë¬¸ìì—´.
    """
    if not openai_client:
        return "GPT í‰ê°€ ì˜¤ë¥˜: OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    # ëª…í™•í•œ JSON í¬ë§·ì„ ìš”êµ¬í•˜ëŠ” í”„ë¡¬í”„íŠ¸
    prompt_text = (
        "ê¸°ì‚¬ ë³¸ë¬¸ì„ ì•„ë˜ ìš”êµ¬ì‚¬í•­ì— ë§ì¶”ì–´ ì‘ë‹µí•´ ì£¼ì„¸ìš”.\n"
        "ìš”êµ¬ì‚¬í•­: 1) í•œêµ­ì–´ë¡œ ìš”ì•½(summary)ë¥¼ 3ë¬¸ì¥ ì´ë‚´ë¡œ ì‘ì„±.\n"
        "2) ì‹ ë¢°ë„(reliability)ëŠ” 'ë†’ìŒ'|'ë³´í†µ'|'ë‚®ìŒ' ì¤‘ í•˜ë‚˜ë¡œ í‰ê°€.\n"
        "3) ì¶”ê°€ ì„¤ëª…(explanation)ì€ ì„ íƒì ìœ¼ë¡œ í¬í•¨ ê°€ëŠ¥.\n"
        "ì‘ë‹µ í˜•ì‹: ë°˜ë“œì‹œ ìœ íš¨í•œ JSON ê°ì²´ë§Œ ë°˜í™˜í•˜ì„¸ìš”. ì˜ˆ: {\"summary\": \"ìš”ì•½ë¬¸\", \"reliability\": \"ë³´í†µ\", \"explanation\": \"ì„¤ëª…\"}\n"
        "ë³¸ë¬¸(ì•„ë˜)ì— ê¸°ë°˜í•´ ì‘ë‹µí•´ ì£¼ì„¸ìš”:\n"
    )

    messages = [
        {"role": "system", "content": "ë‰´ìŠ¤ ê¸°ì‚¬ ìš”ì•½ ë° ì‹ ë¢°ë„ í‰ê°€ë¥¼ JSONìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”."},
        {"role": "user", "content": prompt_text},
        {"role": "user", "content": article_text}
    ]

    try:
        completion = openai_client.chat.completions.create(model=openai_deployment, messages=messages, max_tokens=1024)
        raw_text = completion.choices[0].message.content.strip()
        # ëª¨ë¸ì´ ì¶”ê°€ í…ìŠ¤íŠ¸ë¥¼ ë¶™ì—¬ ë³´ë‚´ëŠ” ê²½ìš° JSON ê°ì²´ë§Œ ì¶”ì¶œí•´ íŒŒì‹± ì‹œë„
        json_match = re.search(r'(\{[\s\S]*\})', raw_text)
        if json_match:
            json_str = json_match.group(1)
            try:
                parsed = json.loads(json_str)
                # ë³´ì¥: summary, reliability í‚¤ê°€ ì¡´ì¬í•˜ë„ë¡ ê¸°ë³¸ê°’ ì ìš©
                summary = parsed.get('summary', '').strip() if isinstance(parsed.get('summary', ''), str) else ''
                reliability = parsed.get('reliability', '').strip() if isinstance(parsed.get('reliability', ''), str) else ''
                return {'summary': summary or 'ìš”ì•½ ì •ë³´ ì—†ìŒ', 'reliability': reliability or 'ì•Œ ìˆ˜ ì—†ìŒ', 'raw': raw_text}
            except Exception as e:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ í´ë°±
                return {'summary': '', 'reliability': '', 'raw': raw_text}
        else:
            # JSON í˜•íƒœë¥¼ ì°¾ì§€ ëª»í•˜ë©´ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ rawë¡œ ë°˜í™˜
            return {'summary': '', 'reliability': '', 'raw': raw_text}
    except Exception as e:
        return f"GPT í‰ê°€ ì˜¤ë¥˜: {e}"

def translate_with_azure(text_to_translate, target_languages):
    if not translator_key: return {lang: "Translation Error" for lang in target_languages}
    headers = {'Ocp-Apim-Subscription-Key': translator_key, 'Ocp-Apim-Subscription-Region': translator_location, 'Content-type': 'application/json', 'X-ClientTraceId': str(uuid.uuid4())}
    params = {'api-version': '3.0', 'from': 'ko', 'to': target_languages}
    body = [{'text': text_to_translate}]
    try:
        response = requests.post(f"{translator_endpoint}/translate", params=params, headers=headers, json=body)
        response.raise_for_status()
        translations = response.json()
        return {t['to']: t['text'] for t in translations[0]['translations']}
    except Exception as e:
        print(f"Azure ë²ˆì—­ API ì˜¤ë¥˜: {e}")
        return {lang: "Translation Error" for lang in target_languages}

# ================================
# 4. ë‰´ìŠ¤ ì²˜ë¦¬ ë° ì €ì¥ ë¡œì§
# ================================
async def process_and_save_news(main_category: str, sub_category: str):
    if not all([container, openai_client, translator_key]):
        print("[ì˜¤ë¥˜] ì„œë¹„ìŠ¤ê°€ ì˜¬ë°”ë¥´ê²Œ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ë‰´ìŠ¤ ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    print(f"[{main_category}] '{sub_category}' ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
    encoded_keyword = quote(sub_category)
    news_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl=ko&gl=KR"
    feed = feedparser.parse(news_url)
    debug = os.getenv('DEBUG_NEWS', 'false').lower() == 'true'

    print(f"[INFO] RSS feed URL: {news_url}")
    print(f"[INFO] feed.entries count: {len(feed.entries)}")
    for i, e in enumerate(feed.entries[:5]):
        try:
            title = getattr(e, 'title', '')
            link = getattr(e, 'link', '')
            print(f"[INFO] entry[{i}] title: {title[:80]} link: {link}")
        except Exception:
            pass

    if debug:
        print(f"[DEBUG] DEBUG_NEWS mode enabled")

    processed_count = 0

    try:
        for entry in feed.entries[:5]:
            try:
                # Early original URL resolution for logging and to avoid later Google-preview images
                original = None
                links = getattr(entry, 'links', None)
                if links:
                    for l in links:
                        href = l.get('href') or l.get('link')
                        if href and href.startswith('http') and 'news.google.com' not in href:
                            original = href
                            break
                if not original and getattr(entry, 'summary', None):
                    soup_summary = BeautifulSoup(entry.summary, 'html.parser')
                    a_tag = soup_summary.find('a', href=True)
                    if a_tag and a_tag['href']:
                        href = a_tag['href']
                        if href.startswith('/'):
                            href = urljoin(getattr(entry, 'link', ''), href)
                        original = href
                # if still not found, try quick requests-based resolution (may perform one request)
                if not original:
                    try:
                        tmp_info = get_original_article_info(getattr(entry, 'link', ''))
                        original = tmp_info.get('original_url')
                    except Exception:
                        original = None

                print(f"[DEBUG] ì‚¬ì „ ì›ë¬¸ ë³€í™˜ ê²°ê³¼: {original}")

                source = getattr(entry, 'source', {}).get('title', 'ì•Œ ìˆ˜ ì—†ìŒ')
                published_time = entry.get('published_parsed')

                if debug:
                    print(f"[DEBUG] Processing entry: {getattr(entry, 'title', '')[:80]}")
                    print(f"[DEBUG] entry.link: {getattr(entry, 'link', '')}")
                    print(f"[DEBUG] source: {source}")
                    print(f"[DEBUG] published_parsed present: {bool(published_time)}")

                if not published_time:
                    if debug:
                        print("[DEBUG] Skipping: no published_parsed")
                    continue

                article_date = datetime.fromtimestamp(time.mktime(published_time))
                if article_date < datetime.now() - timedelta(days=30):
                    if debug:
                        print("[DEBUG] Skipping: older than 30 days")
                    continue

                # Try extracting original URL from RSS entry first
                original = extract_original_from_entry(entry)
                if original:
                    article_info = {'original_url': original, 'image_url': None}
                    if debug:
                        print(f"[DEBUG] original extracted from entry: {original}")
                else:
                    article_info = get_original_article_info(getattr(entry, 'link', ''))
                    if debug:
                        print(f"[DEBUG] resolved original via requests: {article_info.get('original_url')}")

                # Hybrid image extraction
                if not article_info.get('image_url'):
                    hybrid_img = extract_image_hybrid({
                        'link': article_info.get('original_url') or getattr(entry, 'link', ''),
                        'source': source,
                        'media_thumbnail': getattr(entry, 'media_thumbnail', None),
                        'media_content': getattr(entry, 'media_content', None),
                        'summary': getattr(entry, 'summary', None)
                    })
                    if hybrid_img:
                        article_info['image_url'] = hybrid_img
                        if debug:
                            print(f"[DEBUG] hybrid image extraction succeeded: {hybrid_img}")

                # Get content for GPT
                body_text = None
                if article_info.get('original_url') and 'news.google.com' not in article_info.get('original_url'):
                    body_text = scrape_article_body(article_info['original_url'])
                content_for_gpt = body_text if body_text else getattr(entry, 'summary', None)
                if not content_for_gpt:
                    if debug:
                        print("[DEBUG] Skipping: no content_for_gpt")
                    continue

                gpt_result = gpt_evaluate(content_for_gpt, user_selected_sources)
                summary = 'ìš”ì•½ ì •ë³´ ì—†ìŒ'
                reliability = 'ì•Œ ìˆ˜ ì—†ìŒ'
                evaluation_text = ''
                if isinstance(gpt_result, dict):
                    evaluation_text = gpt_result.get('raw', '')
                    summary = gpt_result.get('summary') or summary
                    reliability = gpt_result.get('reliability') or reliability
                else:
                    evaluation_text = str(gpt_result)

                # Translations
                target_languages = ['en', 'ja', 'fr', 'zh-Hans']
                azure_title_translations = translate_with_azure(getattr(entry, 'title', ''), target_languages)
                translated_titles = {lang: text for lang, text in azure_title_translations.items()}
                translated_titles['ko'] = getattr(entry, 'title', '')
                azure_summary_translations = translate_with_azure(summary, target_languages)
                translated_summaries = {lang: text for lang, text in azure_summary_translations.items()}
                translated_summaries['ko'] = summary

                article_id = str(uuid.uuid5(uuid.NAMESPACE_URL, article_info.get('original_url') or getattr(entry, 'link', '')))
                news_item = {
                    'id': article_id,
                    'category': main_category,
                    'subCategory': sub_category,
                    'link': article_info.get('original_url') or getattr(entry, 'link', ''),
                    'source': source,
                    'date': article_date.isoformat(),
                    'summary': summary,
                    'reliability': reliability,
                    'translatedTitles': translated_titles,
                    'translatedSummaries': translated_summaries,
                    'imageUrl': article_info.get('image_url'),
                    'evaluation': evaluation_text
                }

                if debug:
                    try:
                        print("[DEBUG] Upserting news_item:\n" + json.dumps(news_item, ensure_ascii=False, indent=2))
                    except Exception:
                        print("[DEBUG] Upserting news_item (could not serialize to JSON)")

                try:
                    resp = container.upsert_item(body=news_item)
                    if debug:
                        try:
                            print("[DEBUG] Upsert response:\n" + json.dumps(resp, ensure_ascii=False, indent=2))
                        except Exception:
                            print("[DEBUG] Upsert response (non-serializable)")
                    try:
                        read_back = container.read_item(item=article_id, partition_key=news_item.get('category'))
                        if debug:
                            print(f"[DEBUG] Read back item id: {read_back.get('id')}")
                    except Exception as e:
                        if debug:
                            print(f"[DEBUG] Read back failed: {e}")
                except Exception as e:
                    print(f"[ERROR] Upsert failed: {e}")

                processed_count += 1
                await asyncio.sleep(1)

            except asyncio.CancelledError:
                print(f"[ì·¨ì†Œ] ê¸°ì‚¬ ì²˜ë¦¬ ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤. ì¹´í…Œê³ ë¦¬={main_category}, ì†Œë¶„ë¥˜={sub_category}")
                raise
            except Exception as e:
                print(f"  [ì˜¤ë¥˜] ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue

    except asyncio.CancelledError:
        print(f"[ì·¨ì†Œ] ë‰´ìŠ¤ ì²˜ë¦¬ ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤. ì¹´í…Œê³ ë¦¬={main_category}, ì†Œë¶„ë¥˜={sub_category}")
        return
    except Exception as e:
        print(f"[ì˜¤ë¥˜] process_and_save_news ë£¨í”„ ì¤‘ ì˜¤ë¥˜: {e}")

    print(f"  -> [{main_category}] '{sub_category}' ì¹´í…Œê³ ë¦¬ì—ì„œ {processed_count}ê°œ ë‰´ìŠ¤ ì²˜ë¦¬ ì™„ë£Œ.")


async def process_and_save_news_v2(main_category: str, sub_category: str):
    """Clean implementation (v2) that uses the hybrid image extractor and preserves existing behavior.
    This function will be used instead of the older process_and_save_news to avoid indentation issues.
    """
    if not all([container, openai_client, translator_key]):
        print("[ì˜¤ë¥˜] ì„œë¹„ìŠ¤ê°€ ì˜¬ë°”ë¥´ê²Œ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ë‰´ìŠ¤ ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    print(f"[{main_category}] '{sub_category}' ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
    encoded_keyword = quote(sub_category)
    news_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl=ko&gl=KR"
    feed = feedparser.parse(news_url)
    debug = os.getenv('DEBUG_NEWS', 'false').lower() == 'true'

    print(f"[INFO] RSS feed URL: {news_url}")
    print(f"[INFO] feed.entries count: {len(feed.entries)}")

    processed_count = 0

    for entry in feed.entries[:5]:
        try:
            # Early original URL resolution for logging and to avoid later Google-preview images
            original = None
            links = getattr(entry, 'links', None)
            if links:
                for l in links:
                    href = l.get('href') or l.get('link')
                    if href and href.startswith('http') and 'news.google.com' not in href:
                        original = href
                        break
            if not original and getattr(entry, 'summary', None):
                soup_summary = BeautifulSoup(entry.summary, 'html.parser')
                a_tag = soup_summary.find('a', href=True)
                if a_tag and a_tag['href']:
                    href = a_tag['href']
                    if href.startswith('/'):
                        href = urljoin(getattr(entry, 'link', ''), href)
                    original = href
            # if still not found, try quick requests-based resolution (may perform one request)
            if not original:
                try:
                    tmp_info = get_original_article_info(getattr(entry, 'link', ''))
                    original = tmp_info.get('original_url')
                except Exception:
                    original = None

            print(f"[DEBUG] ì‚¬ì „ ì›ë¬¸ ë³€í™˜ ê²°ê³¼: {original}")

            source = getattr(entry, 'source', {}).get('title', 'ì•Œ ìˆ˜ ì—†ìŒ')
            published_time = entry.get('published_parsed')
            if debug:
                print(f"[DEBUG] Processing entry: {getattr(entry, 'title', '')[:80]}")
                print(f"[DEBUG] entry.link: {getattr(entry, 'link', '')}")
                print(f"[DEBUG] source: {source}")
                print(f"[DEBUG] published_parsed present: {bool(published_time)}")
            if not published_time:
                if debug: print("[DEBUG] Skipping: no published_parsed")
                continue
            article_date = datetime.fromtimestamp(time.mktime(published_time))
            if article_date < datetime.now() - timedelta(days=30):
                if debug: print("[DEBUG] Skipping: older than 30 days")
                continue

            # resolve original url
            original = None
            links = getattr(entry, 'links', None)
            if links:
                for l in links:
                    href = l.get('href') or l.get('link')
                    if href and href.startswith('http') and 'news.google.com' not in href:
                        original = href
                        break
            if not original and getattr(entry, 'summary', None):
                soup_summary = BeautifulSoup(entry.summary, 'html.parser')
                a_tag = soup_summary.find('a', href=True)
                if a_tag and a_tag['href']:
                    href = a_tag['href']
                    if href.startswith('/'):
                        href = urljoin(getattr(entry, 'link', ''), href)
                    original = href

            if not original:
                article_info = get_original_article_info(getattr(entry, 'link', ''))
                # print resolved original for immediate visibility
                print(f"[DEBUG] ì‚¬ì „ ì›ë¬¸ ë³€í™˜ ê²°ê³¼(v2): {article_info.get('original_url')}")
            else:
                article_info = {'original_url': original, 'image_url': None}
                print(f"[DEBUG] ì‚¬ì „ ì›ë¬¸ ë³€í™˜ ê²°ê³¼(v2): {original}")

            # hybrid image extraction if needed
            if not article_info.get('image_url'):
                hybrid_img = extract_image_hybrid({
                    'link': article_info.get('original_url') or getattr(entry, 'link', ''),
                    'source': source,
                    'media_thumbnail': getattr(entry, 'media_thumbnail', None),
                    'media_content': getattr(entry, 'media_content', None),
                    'summary': getattr(entry, 'summary', None)
                })
                if hybrid_img:
                    article_info['image_url'] = hybrid_img
                    if debug:
                        print(f"[DEBUG] hybrid image extraction succeeded: {hybrid_img}")

            # ensure we have content for GPT
            body_text = None
            if article_info.get('original_url') and 'news.google.com' not in article_info.get('original_url'):
                body_text = scrape_article_body(article_info['original_url'])
            content_for_gpt = body_text if body_text else getattr(entry, 'summary', None)
            if not content_for_gpt:
                if debug:
                    print("[DEBUG] Skipping: no content_for_gpt")
                continue

            gpt_result = gpt_evaluate(content_for_gpt, user_selected_sources)
            summary = 'ìš”ì•½ ì •ë³´ ì—†ìŒ'
            reliability = 'ì•Œ ìˆ˜ ì—†ìŒ'
            evaluation_text = ''
            if isinstance(gpt_result, dict):
                evaluation_text = gpt_result.get('raw', '')
                summary = gpt_result.get('summary') or summary
                reliability = gpt_result.get('reliability') or reliability
            else:
                evaluation_text = str(gpt_result)

            # translations
            target_languages = ['en', 'ja', 'fr', 'zh-Hans']
            azure_title_translations = translate_with_azure(getattr(entry, 'title', ''), target_languages)
            translated_titles = {lang: text for lang, text in azure_title_translations.items()}
            translated_titles['ko'] = getattr(entry, 'title', '')
            azure_summary_translations = translate_with_azure(summary, target_languages)
            translated_summaries = {lang: text for lang, text in azure_summary_translations.items()}
            translated_summaries['ko'] = summary

            article_id = str(uuid.uuid5(uuid.NAMESPACE_URL, article_info.get('original_url') or getattr(entry, 'link', '')))
            news_item = {
                'id': article_id,
                'category': main_category,
                'subCategory': sub_category,
                'link': article_info.get('original_url') or getattr(entry, 'link', ''),
                'source': source,
                'date': article_date.isoformat(),
                'summary': summary,
                'reliability': reliability,
                'translatedTitles': translated_titles,
                'translatedSummaries': translated_summaries,
                'imageUrl': article_info.get('image_url'),
                'evaluation': evaluation_text
            }

            if debug:
                try:
                    print("[DEBUG] Upserting news_item:\n" + json.dumps(news_item, ensure_ascii=False, indent=2))
                except Exception:
                    print("[DEBUG] Upserting news_item (could not serialize to JSON)")

            try:
                resp = container.upsert_item(body=news_item)
                if debug:
                    try:
                        print("[DEBUG] Upsert response:\n" + json.dumps(resp, ensure_ascii=False, indent=2))
                    except Exception:
                        print("[DEBUG] Upsert response (non-serializable)")
                try:
                    read_back = container.read_item(item=article_id, partition_key=news_item.get('category'))
                    if debug:
                        print(f"[DEBUG] Read back item id: {read_back.get('id')}")
                except Exception as e:
                    if debug:
                        print(f"[DEBUG] Read back failed: {e}")
            except Exception as e:
                print(f"[ERROR] Upsert failed: {e}")

            processed_count += 1
            await asyncio.sleep(1)

        except asyncio.CancelledError:
            print(f"[ì·¨ì†Œ] ê¸°ì‚¬ ì²˜ë¦¬ ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤. ì¹´í…Œê³ ë¦¬={main_category}, ì†Œë¶„ë¥˜={sub_category}")
            raise
        except Exception as e:
            print(f"  [ì˜¤ë¥˜] ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue

    print(f"  -> [{main_category}] '{sub_category}' ì¹´í…Œê³ ë¦¬ì—ì„œ {processed_count}ê°œ ë‰´ìŠ¤ ì²˜ë¦¬ ì™„ë£Œ.")

# ================================
# 5. ë°±ê·¸ë¼ìš´ë“œ ë° ì‹œì‘ ì´ë²¤íŠ¸
# ================================
async def fetch_all_categories_in_background():
    print("ğŸš€ ì„œë²„ ì‹œì‘ - ë°±ê·¸ë¼ìš´ë“œ ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.")


    # ëª¨ë“  ì¹´í…Œê³ ë¦¬ì™€ ì†Œë¶„ë¥˜ë¥¼ ìˆœíšŒí•˜ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    # ë¡œì»¬ í…ŒìŠ¤íŠ¸ ëª©ì ì˜ ì œí•œì€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.
    for main_category, sub_categories in categories.items():
        for sub_category in sub_categories:
            await process_and_save_news_v2(main_category, sub_category)
    print("âœ… ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

@app.on_event("startup")
async def startup_event():
    print("ì„œë²„ ì‹œì‘: ë°±ê·¸ë¼ìš´ë“œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‘ì—…ì„ ì˜ˆì•½í•©ë‹ˆë‹¤.")
    # use new v2 processor
    asyncio.create_task(fetch_all_categories_in_background())

# ================================
# 6. API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜
# ================================
@app.get("/api/news", response_model=List[NewsArticle])
def get_news(category: Optional[str] = None, limit: int = 50):
    if not container:
        raise HTTPException(status_code=500, detail="Cosmos DB ì»¨í…Œì´ë„ˆê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    query = "SELECT * FROM c ORDER BY c.date DESC OFFSET 0 LIMIT @limit"
    params = [{"name": "@limit", "value": limit}]
    if category and category != 'all':
        query = "SELECT * FROM c WHERE c.category = @category ORDER BY c.date DESC OFFSET 0 LIMIT @limit"
        params.append({"name": "@category", "value": category})
    try:
        items = list(container.query_items(query=query, parameters=params, enable_cross_partition_query=False if category and category != 'all' else True))
        return items
    except exceptions.CosmosResourceNotFoundError:
        return []
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë‰´ìŠ¤ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


@app.get("/api/original")
def api_get_original(url: str):
    """ì£¼ì–´ì§„ Google News RSS/article URLì—ì„œ ì›ë¬¸(original_url)ê³¼ ì¶”ì¶œëœ ì´ë¯¸ì§€(image_url)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì‚¬ìš©ë²•: GET /api/original?url={GOOGLE_NEWS_URL}
    """
    if not url:
        raise HTTPException(status_code=400, detail="url ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    try:
        result = get_original_article_info(url)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì›ë¬¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")


@app.post("/api/news/sample")
def insert_sample_news():
    """ìƒ˜í”Œ ë‰´ìŠ¤ í•­ëª©ì„ ìƒì„±í•˜ì—¬ Cosmosì— ì—…ì„œíŠ¸í•©ë‹ˆë‹¤. (ë¡œì»¬ í…ŒìŠ¤íŠ¸ ìš©)"""
    if not container:
        raise HTTPException(status_code=500, detail="Cosmos DB ì»¨í…Œì´ë„ˆê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    try:
        sample_id = str(uuid.uuid4())
        sample_item = {
            'id': sample_id,
            'category': 'í…ŒìŠ¤íŠ¸',
            'subCategory': 'ìƒ˜í”Œ',
            'link': 'https://example.com/sample-article',
            'source': 'ìƒ˜í”Œì–¸ë¡ ì‚¬',
            'date': datetime.now().isoformat(),
            'summary': 'ì´ê²ƒì€ ìƒ˜í”Œ ìš”ì•½ì…ë‹ˆë‹¤.',
            'reliability': 'ë³´í†µ',
            'translatedTitles': {'ko': 'ìƒ˜í”Œ ì œëª©', 'en': 'Sample Title', 'ja': 'ã‚µãƒ³ãƒ—ãƒ«', 'fr': 'Exemple', 'zh-Hans': 'ç¤ºä¾‹'},
            'translatedSummaries': {'ko': 'ì´ê²ƒì€ ìƒ˜í”Œ ìš”ì•½ì…ë‹ˆë‹¤.', 'en': 'This is a sample summary.', 'ja': 'ã“ã‚Œã¯ã‚µãƒ³ãƒ—ãƒ«ã®è¦ç´„ã§ã™ã€‚', 'fr': 'Ceci est un rÃ©sumÃ© exemple.', 'zh-Hans': 'è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹æ‘˜è¦ã€‚'},
            'imageUrl': None,
            'evaluation': 'ìƒ˜í”Œ GPT í‰ê°€ í…ìŠ¤íŠ¸'
        }
        container.upsert_item(body=sample_item)
        return {"status": "ok", "id": sample_id}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ìƒ˜í”Œ ì‚½ì… ì¤‘ ì˜¤ë¥˜: {e}")

@app.get("/")
def read_root():
    return {"message": "NewsFlow AI ë°±ì—”ë“œ ì„œë²„ì…ë‹ˆë‹¤. /docs ë¡œ API ë¬¸ì„œë¥¼ í™•ì¸í•˜ì„¸ìš”."}