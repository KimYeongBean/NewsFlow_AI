import feedparser
import time
import os
from datetime import datetime, timedelta
from urllib.parse import quote
from openai import AzureOpenAI
import re  # ğŸ‘ˆ ì •ê·œ í‘œí˜„ì‹ ì‚¬ìš©ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€

# ================================
# 1. ì‚¬ìš©ì ì„¤ì •
# ================================
user_selected_sources = ["ì¡°ì„ ì¼ë³´", "í•œê²¨ë ˆ", "ì¤‘ì•™ì¼ë³´", "ë™ì•„ì¼ë³´", "ê²½í–¥ì‹ ë¬¸"]
user_follow_categories = ["ì •ì¹˜", "ê²½ì œ"]

# ================================
# 2. ì „ì²´ ë‰´ìŠ¤/ì¹´í…Œê³ ë¦¬
# ================================
all_sources = [
    'MBCë‰´ìŠ¤', 'ì—°í•©ë‰´ìŠ¤', 'ì¡°ì„ ì¼ë³´', 'ë‰´ìŠ¤1', 'JTBC ë‰´ìŠ¤',
    'ì¤‘ì•™ì¼ë³´', 'SBS ë‰´ìŠ¤', 'YTN', 'í•œê²¨ë ˆ', 'ê²½í–¥ì‹ ë¬¸',
    'ì˜¤ë§ˆì´ë‰´ìŠ¤', 'í•œêµ­ê²½ì œ'
]

categories = {
    'ì •ì¹˜': ['ëŒ€í†µë ¹ì‹¤', 'êµ­íšŒ', 'ì •ë‹¹', 'í–‰ì •', 'ì™¸êµ', 'êµ­ë°©/ë¶í•œ'],
    'ê²½ì œ': ['ê¸ˆìœµ/ì¦ê¶Œ', 'ì‚°ì—…/ì¬ê³„', 'ì¤‘ê¸°/ë²¤ì²˜', 'ë¶€ë™ì‚°', 'ê¸€ë¡œë²Œ', 'ìƒí™œ'],
}

MAX_ARTICLES_PER_CATEGORY = 100
save_path = 'D:/í•œêµ­ê¸°ìˆ êµìœ¡ëŒ€í•™êµ/2025ë…„ 2í•™ë…„ ì—¬ë¦„ ê³„ì ˆ/ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ AI/í…€í”„/output'
one_month_ago = datetime.now() - timedelta(days=30)
os.makedirs(save_path, exist_ok=True)

# ================================
# 3. Azure OpenAI ì´ˆê¸°í™”
# ================================
endpoint = "https://newscheck2.openai.azure.com/"
deployment = "gpt-5-nano"
subscription_key = "Dsf5DmuTn1cS7lXaSxSTnO30kTZCqr2xKqIjLwvdovEGnQsz3NjlJQQJ99BHACHYHv6XJ3w3AAABACOGJk53"

client = AzureOpenAI(
    azure_endpoint=endpoint,
    api_key=subscription_key,
    api_version="2025-01-01-preview",
)

# ================================
# 4. ë‰´ìŠ¤ ìˆ˜ì§‘ í•¨ìˆ˜
# ================================
def fetch_news(sub_category, main_category):
    encoded_keyword = quote(sub_category)
    news_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl=ko&gl=KR"
    feed = feedparser.parse(news_url)

    articles = []
    saved_count = 0

    for entry in feed.entries:
        if saved_count >= MAX_ARTICLES_PER_CATEGORY:
            break

        source_name = getattr(entry, 'source', None)
        if source_name and source_name.title in all_sources:
            published_time = entry.get('published_parsed')
            if not published_time:
                continue
            article_date = datetime.fromtimestamp(time.mktime(published_time))
            if article_date < one_month_ago:
                continue

            articles.append({
                'title': entry.title,
                'link': entry.link,
                'source': source_name.title,
                'date': article_date.strftime('%Y-%m-%d %H:%M:%S'),
                'content': entry.title
            })
            saved_count += 1

    return articles

# ================================
# 5. GPT í‰ê°€ í•¨ìˆ˜
# ================================
def gpt_evaluate(article_text, selected_sources):
    prompt_text = f"""
ë‹¹ì‹ ì€ ë‰´ìŠ¤ ìš”ì•½ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìê°€ ì„ íƒí•œ ì–¸ë¡ ì‚¬: {', '.join(selected_sources)}

ì•„ë˜ ë‰´ìŠ¤ ì œëª© ë˜ëŠ” ë³¸ë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ:
1) 3ì¤„ë¡œ ìš”ì•½
2) ì„ íƒí•œ ì–¸ë¡ ì‚¬ì™€ í•µì‹¬ ì£¼ì¥ ë¹„êµ
3) ì‹ ë¢°ë„ ë“±ê¸‰ ì¶œë ¥ (ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ë§Œ ì‚¬ìš©):
    ì‹ ë¢°ë„: ë†’ìŒ / ë³´í†µ / ë‚®ìŒ

ì‹ ë¢°ë„ í‰ê°€ ê¸°ì¤€:
- ì£¼ìš” ì–¸ë¡ ì‚¬(ì¡°ì„ ì¼ë³´, í•œê²¨ë ˆ, ì¤‘ì•™ì¼ë³´, ë™ì•„ì¼ë³´, ê²½í–¥ì‹ ë¬¸) â†’ ë†’ìŒ
- ì œëª©ë§Œ ì¡´ì¬í•˜ê±°ë‚˜ ì¼ë¶€ ì •ë³´ë§Œ ìˆëŠ” ê²½ìš° â†’ ë³´í†µ
- ê·¼ê±° ë¶€ì¡±/ì„ ì •ì /ì¶œì²˜ ë¶ˆë¶„ëª… â†’ ë‚®ìŒ

âš ï¸ ì¶œë ¥ í˜•ì‹ì„ ë°˜ë“œì‹œ ì§€ì¼œì£¼ì„¸ìš”.
"""

    messages = [
        {"role": "system", "content": "ë„ˆëŠ” ë‰´ìŠ¤ ìš”ì•½ê³¼ ì–¸ë¡ ì‚¬ ë¹„êµ, ì‹ ë¢°ë„ í‰ê°€ë§Œ ê°„ë‹¨íˆ ì¶œë ¥í•˜ëŠ” ë„ìš°ë¯¸ì•¼."},
        {"role": "user", "content": prompt_text},
        {"role": "user", "content": article_text}
    ]

    try:
        completion = client.chat.completions.create(
            model=deployment,
            messages=messages,
            max_completion_tokens=1024
        )
        result_text = completion.choices[0].message.content.strip()
        return result_text
    except Exception as e:
        return f"GPT í‰ê°€ ì˜¤ë¥˜: {e}"

# ================================
# 6. HTML ì €ì¥ í•¨ìˆ˜ (ìˆ˜ì •)
# ================================
def save_news(main_category, sub_category, articles):
    main_path = os.path.join(save_path, main_category)
    os.makedirs(main_path, exist_ok=True)
    file_name = f"{sub_category.replace('/', '_')}_news.html"
    full_path = os.path.join(main_path, file_name)

    with open(full_path, 'w', encoding='utf-8') as f:
        f.write("<html><head><meta charset='utf-8'>")
        f.write("<style>")
        f.write("body { font-family: Arial, sans-serif; }")
        f.write("h1 { color: #2E86C1; }")
        f.write("h3 { margin-bottom: 0; }")
        f.write("p { margin-top: 5px; margin-bottom: 5px; }")
        f.write("hr { border: 1px solid #ccc; }")
        f.write(".summary { color: #4CAF50; }")
        f.write(".reliability { font-weight: bold; }")
        f.write(".high { color: #28a745; }")
        f.write(".medium { color: #ffc107; }")
        f.write(".low { color: #dc3545; }")
        f.write("</style></head><body>\n")

        f.write(f"<h1>{main_category} - {sub_category}</h1>\n")

        for article in articles:
            evaluation_text = article.get('evaluation', '')
            
            summary = ""
            reliability = ""
            
            # ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•´ ìš”ì•½ê³¼ ì‹ ë¢°ë„ ë“±ê¸‰ ì¶”ì¶œ
            summary_match = re.search(r'1\)(.*?)(?=2\)|\Z)', evaluation_text, re.DOTALL)
            reliability_match = re.search(r'ì‹ ë¢°ë„:\s*(ë†’ìŒ|ë³´í†µ|ë‚®ìŒ)', evaluation_text)

            if summary_match:
                summary = summary_match.group(1).strip().replace("\n", "<br>")
            
            if reliability_match:
                reliability = reliability_match.group(1).strip()
            
            f.write(f"<h3><a href='{article['link']}' target='_blank'>{article['title']}</a></h3>\n")
            f.write(f"<p><b>ì–¸ë¡ ì‚¬:</b> {article['source']} | <b>ë°œí–‰ ì‹œê°„:</b> {article['date']}</p>\n")
            
            # ìš”ì•½ ë‚´ìš©ê³¼ ì‹ ë¢°ë„ ë“±ê¸‰ì„ í•˜ë‚˜ì˜ ì¤„ì— í‘œì‹œ
            if summary:
                if "ë†’ìŒ" in reliability:
                    reliability_class = "high"
                elif "ë³´í†µ" in reliability:
                    reliability_class = "medium"
                else:
                    reliability_class = "low"
                    
                f.write(f"<p class='summary'><b>3ì¤„ ìš”ì•½:</b> {summary} <span class='reliability {reliability_class}'>ì‹ ë¢°ë„: {reliability}</span></p>\n")
            
            f.write("<hr>\n")

        f.write("</body></html>")

# ================================
# 7. ë©”ì¸ ì‹¤í–‰ ë£¨í”„
# ================================
for main_category, sub_categories in categories.items():
    if main_category not in user_follow_categories:
        continue

    for sub_category in sub_categories:
        print(f"[{main_category}] {sub_category} ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        articles = fetch_news(sub_category, main_category)

        for article in articles:
            article['evaluation'] = gpt_evaluate(article['content'], user_selected_sources)
            time.sleep(1)  # ì„œë²„ ë¶€ë‹´ ë°©ì§€

        save_news(main_category, sub_category, articles)
        print(f"  -> {len(articles)}ê°œ ë‰´ìŠ¤ ì €ì¥ ì™„ë£Œ")

print("\nğŸ‰ ëª¨ë“  ë‰´ìŠ¤ ìˆ˜ì§‘ ë° GPT í‰ê°€ ì™„ë£Œ!")
