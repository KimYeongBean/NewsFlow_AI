import feedparser                 # ì›¹ì˜ RSS/Atom í”¼ë“œë¥¼ íŒŒì‹±í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬
import time                        # ì‹œê°„ ê´€ë ¨ í•¨ìˆ˜ë¥¼ ì œê³µí•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬
import os                          # ìš´ì˜ì²´ì œì™€ ìƒí˜¸ì‘ìš©í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬
from datetime import datetime, timedelta # ë‚ ì§œì™€ ì‹œê°„ì„ ë‹¤ë£¨ëŠ” ëª¨ë“ˆ
from urllib.parse import quote     # URLì— í¬í•¨ë  ìˆ˜ ìˆëŠ” íŠ¹ìˆ˜ ë¬¸ìë¥¼ ì¸ì½”ë”©í•˜ëŠ” í•¨ìˆ˜
from openai import AzureOpenAI     # Azure OpenAI APIë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import re                          # ì •ê·œ í‘œí˜„ì‹ ë¼ì´ë¸ŒëŸ¬ë¦¬

# ================================
# 1. ì‚¬ìš©ì ì„¤ì •
# ================================
user_selected_sources = ["ì¡°ì„ ì¼ë³´", "í•œê²¨ë ˆ", "ì¤‘ì•™ì¼ë³´", "ë™ì•„ì¼ë³´", "ê²½í–¥ì‹ ë¬¸"] # GPTê°€ ì‹ ë¢°ë„ë¥¼ í‰ê°€í•  ë•Œ ê¸°ì¤€ìœ¼ë¡œ ì‚¼ì„ ì–¸ë¡ ì‚¬ ëª©ë¡

#ìœ„ì— ë‚´ìš©ì€ ì‚¬ìš©ì ì§ì ‘ ì„¤ì •í•  ìˆ˜ ìˆê²Œ ë³€ê²½í•´ì•¼í•œë‹¤.(23ë²ˆì¤„ ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ì—ì„œ ì§ì ‘ ì„¤ì •)

user_follow_categories = ["ì •ì¹˜", "ê²½ì œ"] # ì‚¬ìš©ìê°€ ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì›í•˜ëŠ” ëŒ€ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬

#ìœ„ì— ë‚´ìš©ì€ ì‚¬ìš©ìê°€ ì§ì ‘ ì„¤ì •í•  ìˆ˜ ìˆê²Œ ë³€ê²½í•´ì•¼í•œë‹¤.(29ë²ˆì¤„ ì¹´í…Œì½”ë¦¬ì—ì„œ ì§ì ‘ ì„¤ì •)

# ================================
# 2. ì „ì²´ ë‰´ìŠ¤/ì¹´í…Œê³ ë¦¬
# ================================
all_sources = [
    'MBCë‰´ìŠ¤', 'ì—°í•©ë‰´ìŠ¤', 'ì¡°ì„ ì¼ë³´', 'ë‰´ìŠ¤1', 'JTBC ë‰´ìŠ¤',
    'ì¤‘ì•™ì¼ë³´', 'SBS ë‰´ìŠ¤', 'YTN', 'í•œê²¨ë ˆ', 'ê²½í–¥ì‹ ë¬¸',
    'ì˜¤ë§ˆì´ë‰´ìŠ¤', 'í•œêµ­ê²½ì œ'
] # ìˆ˜ì§‘ ëŒ€ìƒì— í¬í•¨ë  ìˆ˜ ìˆëŠ” ì „ì²´ ì–¸ë¡ ì‚¬ ëª©ë¡

categories = {
    'ì •ì¹˜': ['ëŒ€í†µë ¹ì‹¤', 'êµ­íšŒ', 'ì •ë‹¹', 'í–‰ì •', 'ì™¸êµ', 'êµ­ë°©/ë¶í•œ'],
    'ê²½ì œ': ['ê¸ˆìœµ/ì¦ê¶Œ', 'ì‚°ì—…/ì¬ê³„', 'ì¤‘ê¸°/ë²¤ì²˜', 'ë¶€ë™ì‚°', 'ê¸€ë¡œë²Œ', 'ìƒí™œ'],
    'ì‚¬íšŒ': ['ì‚¬ê±´ì‚¬ê³ ', 'êµìœ¡', 'ë…¸ë™', 'ì–¸ë¡ ', 'í™˜ê²½', 'ì¸ê¶Œ/ë³µì§€', 'ì‹í’ˆ/ì˜ë£Œ', 'ì§€ì—­', 'ì¸ë¬¼'],
    'IT_ê³¼í•™': ['ëª¨ë°”ì¼', 'ì¸í„°ë„·/SNS', 'í†µì‹ /ë‰´ë¯¸ë””ì–´', 'IT', 'ë³´ì•ˆ/í•´í‚¹', 'ì»´í“¨í„°', 'ê²Œì„/ë¦¬ë·°', 'ê³¼í•™'],
    'ìƒí™œ_ë¬¸í™”': ['ê±´ê°•', 'ìë™ì°¨', 'ì—¬í–‰/ë ˆì €', 'ìŒì‹/ë§›ì§‘', 'íŒ¨ì…˜/ë·°í‹°', 'ê³µì—°/ì „ì‹œ', 'ì±…', 'ì¢…êµ', 'ë‚ ì”¨', 'ìƒí™œ'],
    'ì„¸ê³„': ['ì•„ì‹œì•„/í˜¸ì£¼', 'ë¯¸êµ­/ì¤‘ë‚¨ë¯¸', 'ìœ ëŸ½', 'ì¤‘ë™/ì•„í”„ë¦¬ì¹´', 'ì„¸ê³„']
}

MAX_ARTICLES_PER_CATEGORY = 100 # í•œ ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ë‹¹ ìˆ˜ì§‘í•  ìµœëŒ€ ê¸°ì‚¬ ìˆ˜
save_path = 'D:/í•œêµ­ê¸°ìˆ êµìœ¡ëŒ€í•™êµ/2025ë…„ 2í•™ë…„ ì—¬ë¦„ ê³„ì ˆ/ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ AI/í…€í”„/output' # ë‰´ìŠ¤ë¥¼ ì €ì¥í•  í´ë” ê²½ë¡œ

#ì €ì¥ëœ íŒ¨ì¹˜ í´ë”ëŠ” azureë¡œ ë°›ì•„ì•¼ í•˜ê¸° ë•Œë¬¸ì— ì„ì˜ë¡œ ì•„ì›ƒí’‹ì ì§€ì •í•œ ê±°ë‹¤.

one_month_ago = datetime.now() - timedelta(days=30) # í˜„ì¬ ì‹œê°„ìœ¼ë¡œë¶€í„° 30ì¼ ì „ì˜ ë‚ ì§œë¥¼ ê³„ì‚°
os.makedirs(save_path, exist_ok=True) # ì €ì¥ ê²½ë¡œì˜ ë””ë ‰í„°ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±í•˜ê³ , ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë¬´ì‹œ

# ================================
# 3. Azure OpenAI ì´ˆê¸°í™”
# ================================
endpoint = "https://newscheck2.openai.azure.com/" # Azure OpenAI ì„œë¹„ìŠ¤ì˜ ì ‘ì† ì£¼ì†Œ
deployment = "gpt-5-nano"                         # ì‚¬ìš©í•  ëª¨ë¸ì˜ ì´ë¦„
subscription_key = "Dsf5DmuTn1cS7lXaSxSTnO30kTZCqr2xKqIjLwvdovEGnQsz3NjlJQQJ99BHACHYHv6XJ3w3AAABACOGJk53" # APIë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì¸ì¦ í‚¤

client = AzureOpenAI(                             # Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ê°ì²´ ìƒì„±
    azure_endpoint=endpoint,
    api_key=subscription_key,
    api_version="2025-01-01-preview",
)

# ================================
# 4. ë‰´ìŠ¤ ìˆ˜ì§‘ í•¨ìˆ˜
# ================================
def fetch_news(sub_category, main_category): # ë‰´ìŠ¤ ìˆ˜ì§‘ í•¨ìˆ˜ ì •ì˜
    encoded_keyword = quote(sub_category)  # í•œê¸€ í‚¤ì›Œë“œë¥¼ URL í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    news_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl=ko&gl=KR" # êµ¬ê¸€ ë‰´ìŠ¤ RSS ê²€ìƒ‰ URL ìƒì„±
    feed = feedparser.parse(news_url)      # URLì˜ RSS í”¼ë“œë¥¼ íŒŒì‹±í•˜ì—¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°

    articles = []                          # ìˆ˜ì§‘ëœ ê¸°ì‚¬ë¥¼ ì €ì¥í•  ë¹ˆ ë¦¬ìŠ¤íŠ¸
    saved_count = 0                        # ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜ë¥¼ ì„¸ëŠ” ì¹´ìš´í„°

    for entry in feed.entries:             # RSS í”¼ë“œì˜ ê° ê¸°ì‚¬ í•­ëª©ì— ëŒ€í•´ ë°˜ë³µ
        if saved_count >= MAX_ARTICLES_PER_CATEGORY: # ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜ê°€ ìµœëŒ€ì¹˜ë¥¼ ë„˜ìœ¼ë©´
            break                          # ë°˜ë³µë¬¸ì„ ì¤‘ë‹¨

        source_name = getattr(entry, 'source', None) # ê¸°ì‚¬ í•­ëª©ì—ì„œ ì–¸ë¡ ì‚¬ ì •ë³´ë¥¼ ê°€ì ¸ì˜´
        if source_name and source_name.title in all_sources: # ì–¸ë¡ ì‚¬ ì´ë¦„ì´ ìˆê³ , ì „ì²´ ì–¸ë¡ ì‚¬ ëª©ë¡ì— ìˆìœ¼ë©´
            published_time = entry.get('published_parsed')   # ê¸°ì‚¬ ë°œí–‰ ì‹œê°„ì„ ê°€ì ¸ì˜´
            if not published_time:         # ë°œí–‰ ì‹œê°„ ì •ë³´ê°€ ì—†ìœ¼ë©´
                continue                   # ë‹¤ìŒ ê¸°ì‚¬ë¡œ ë„˜ì–´ê°
            article_date = datetime.fromtimestamp(time.mktime(published_time)) # ë°œí–‰ ì‹œê°„ì„ datetime ê°ì²´ë¡œ ë³€í™˜
            if article_date < one_month_ago: # ê¸°ì‚¬ ë°œí–‰ì¼ì´ í•œ ë‹¬ë³´ë‹¤ ì˜¤ë˜ë˜ì—ˆìœ¼ë©´
                continue                   # ë‹¤ìŒ ê¸°ì‚¬ë¡œ ë„˜ì–´ê°

            articles.append({              # í•„í„°ë§ëœ ê¸°ì‚¬ ì •ë³´ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                'title': entry.title,
                'link': entry.link,
                'source': source_name.title,
                'date': article_date.strftime('%Y-%m-%d %H:%M:%S'),
                'content': entry.title
            })
            saved_count += 1               # ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜ ì¦ê°€

    return articles                        # ìˆ˜ì§‘ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

# ================================
# 5. GPT í‰ê°€ í•¨ìˆ˜
# ================================
def gpt_evaluate(article_text, selected_sources): # GPT í‰ê°€ í•¨ìˆ˜ ì •ì˜
    prompt_text = f"""
ë‹¹ì‹ ì€ ë‰´ìŠ¤ ìš”ì•½ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìê°€ ì„ íƒí•œ ì–¸ë¡ ì‚¬: {', '.join(selected_sources)}

ì•„ë˜ ë‰´ìŠ¤ ì œëª© ë˜ëŠ” ë³¸ë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ:
1) 3ì¤„ë¡œ ìš”ì•½
2) ì„ íƒí•œ ì–¸ë¡ ì‚¬ì™€ í•µì‹¬ ì£¼ì¥ ë¹„êµ
3) ì‹ ë¢°ë„ ë“±ê¸‰ ì¶œë ¥ (ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ë§Œ ì‚¬ìš©):
    ì‹ ë¢°ë„: ë†’ìŒ / ë³´í†µ / ë‚®ìŒ

ì‹ ë¢°ë„ í‰ê°€ ê¸°ì¤€:
- ì£¼ìš” ì–¸ë¡ ì‚¬(ì¡°ì„ ì¼ë³´, í•œê²¨ë ˆ, ì¤‘ì•™ì¼ë³´, ë™ì•„ì¼ë³´, ê²½í–¥ì‹ ë¬¸) â†’ ë†’ìŒ
- ì œëª©ë§Œ ì¡´ì¬í•˜ê±°ë‚˜ ì¼ë¶€ ì •ë³´ë§Œ ìˆëŠ” ê²½ìš° â†’ ë³´í†µ
- ê·¼ê±° ë¶€ì¡±/ì„ ì •ì /ì¶œì²˜ ë¶ˆë¶„ëª… â†’ ë‚®ìŒ

âš ï¸ ì¶œë ¥ í˜•ì‹ì„ ë°˜ë“œì‹œ ì§€ì¼œì£¼ì„¸ìš”.
""" # GPTì—ê²Œ ìš”ì²­í•  í”„ë¡¬í”„íŠ¸(ì§€ì‹œì‚¬í•­) ì •ì˜

    messages = [
        {"role": "system", "content": "ë„ˆëŠ” ë‰´ìŠ¤ ìš”ì•½ê³¼ ì–¸ë¡ ì‚¬ ë¹„êµ, ì‹ ë¢°ë„ í‰ê°€ë§Œ ê°„ë‹¨íˆ ì¶œë ¥í•˜ëŠ” ë„ìš°ë¯¸ì•¼."}, # AIì˜ ì—­í• ì„ ì„¤ì •
        {"role": "user", "content": prompt_text}, # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ì „ë‹¬
        {"role": "user", "content": article_text} # í‰ê°€í•  ê¸°ì‚¬ ë‚´ìš© ì „ë‹¬
    ]

    try:                                   # ì˜ˆì™¸ ì²˜ë¦¬ ì‹œì‘
        completion = client.chat.completions.create( # GPT API í˜¸ì¶œ
            model=deployment,
            messages=messages,
            max_completion_tokens=1024
        )
        result_text = completion.choices[0].message.content.strip() # GPT ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        return result_text                 # ê²°ê³¼ í…ìŠ¤íŠ¸ ë°˜í™˜
    except Exception as e:                 # API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ
        return f"GPT í‰ê°€ ì˜¤ë¥˜: {e}"      # ì˜¤ë¥˜ ë©”ì‹œì§€ ë°˜í™˜

# ================================
# 6. HTML ì €ì¥ í•¨ìˆ˜ (ìˆ˜ì •)
# ================================
def save_news(main_category, sub_category, articles): # HTML ì €ì¥ í•¨ìˆ˜ ì •ì˜
    main_path = os.path.join(save_path, main_category) # ëŒ€ë¶„ë¥˜ ì´ë¦„ìœ¼ë¡œ í´ë” ê²½ë¡œ ìƒì„±
    os.makedirs(main_path, exist_ok=True)  # í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
    file_name = f"{sub_category.replace('/', '_')}_news.html" # íŒŒì¼ ì´ë¦„ ìƒì„±
    full_path = os.path.join(main_path, file_name) # ì „ì²´ íŒŒì¼ ê²½ë¡œ ìƒì„±

    with open(full_path, 'w', encoding='utf-8') as f: # HTML íŒŒì¼ì„ ì“°ê¸° ëª¨ë“œë¡œ ì—´ê¸°
        f.write("<html><head><meta charset='utf-8'>") # HTML ê¸°ë³¸ êµ¬ì¡° ì‘ì„±
        f.write("<style>")                 # CSS ìŠ¤íƒ€ì¼ ì‹œì‘
        f.write("...")                     # ë³¸ë¬¸, ì œëª© ë“±ì˜ CSS ìŠ¤íƒ€ì¼ ì •ì˜
        f.write("</style></head><body>\n") # CSS ìŠ¤íƒ€ì¼ ë, ë³¸ë¬¸ ì‹œì‘

        f.write(f"<h1>{main_category} - {sub_category}</h1>\n") # í˜ì´ì§€ ì œëª© ì‘ì„±

        for article in articles:           # ê° ê¸°ì‚¬ì— ëŒ€í•´ ë°˜ë³µ
            evaluation_text = article.get('evaluation', '') # ê¸°ì‚¬ì˜ GPT í‰ê°€ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            
            summary = ""                   # ìš”ì•½ì„ ë‹´ì„ ë³€ìˆ˜
            reliability = ""               # ì‹ ë¢°ë„ë¥¼ ë‹´ì„ ë³€ìˆ˜
            
            summary_match = re.search(r'1\)(.*?)(?=2\)|\Z)', evaluation_text, re.DOTALL) # ì •ê·œ í‘œí˜„ì‹ìœ¼ë¡œ ìš”ì•½ ì¶”ì¶œ
            reliability_match = re.search(r'ì‹ ë¢°ë„:\s*(ë†’ìŒ|ë³´í†µ|ë‚®ìŒ)', evaluation_text) # ì •ê·œ í‘œí˜„ì‹ìœ¼ë¡œ ì‹ ë¢°ë„ ë“±ê¸‰ ì¶”ì¶œ

            if summary_match:              # ìš”ì•½ ë‚´ìš©ì´ ìˆìœ¼ë©´
                summary = summary_match.group(1).strip().replace("\n", "<br>") # í…ìŠ¤íŠ¸ ì •ë¦¬ ë° ì¤„ë°”ê¿ˆ ì²˜ë¦¬
            
            if reliability_match:          # ì‹ ë¢°ë„ ë“±ê¸‰ì´ ìˆìœ¼ë©´
                reliability = reliability_match.group(1).strip() # ì‹ ë¢°ë„ ë“±ê¸‰ ì¶”ì¶œ
            
            f.write(f"<h3><a href='{article['link']}' target='_blank'>{article['title']}</a></h3>\n") # ê¸°ì‚¬ ì œëª©ê³¼ ë§í¬ ì‘ì„±
            f.write(f"<p><b>ì–¸ë¡ ì‚¬:</b> {article['source']} | <b>ë°œí–‰ ì‹œê°„:</b> {article['date']}</p>\n") # ì–¸ë¡ ì‚¬ì™€ ë°œí–‰ ì‹œê°„ ì‘ì„±
            
            if summary:                    # ìš”ì•½ ë‚´ìš©ì´ ìˆìœ¼ë©´
                if "ë†’ìŒ" in reliability:
                    reliability_class = "high" # ì‹ ë¢°ë„ 'ë†’ìŒ'ì¼ ê²½ìš° CSS í´ë˜ìŠ¤ ì§€ì •
                elif "ë³´í†µ" in reliability:
                    reliability_class = "medium" # ì‹ ë¢°ë„ 'ë³´í†µ'ì¼ ê²½ìš° CSS í´ë˜ìŠ¤ ì§€ì •
                else:
                    reliability_class = "low" # ì‹ ë¢°ë„ 'ë‚®ìŒ'ì¼ ê²½ìš° CSS í´ë˜ìŠ¤ ì§€ì •
                    
                f.write(f"<p class='summary'><b>3ì¤„ ìš”ì•½:</b> {summary} <span class='reliability {reliability_class}'>ì‹ ë¢°ë„: {reliability}</span></p>\n") # ìš”ì•½ ë° ì‹ ë¢°ë„ ì‘ì„±
            
            f.write("<hr>\n")              # ê¸°ì‚¬ ì‚¬ì´ì— ê°€ë¡œì„  ì¶”ê°€

        f.write("</body></html>")          # HTML ë³¸ë¬¸ ë

# ================================
# 7. ë©”ì¸ ì‹¤í–‰ ë£¨í”„
# ================================
for main_category, sub_categories in categories.items(): # ëª¨ë“  ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ ë°˜ë³µ
    if main_category not in user_follow_categories: # ì‚¬ìš©ìê°€ ì„ íƒí•œ ì¹´í…Œê³ ë¦¬ê°€ ì•„ë‹ˆë©´
        continue                                   # ë‹¤ìŒ ë°˜ë³µìœ¼ë¡œ ë„˜ì–´ê°

    for sub_category in sub_categories:            # ê° ëŒ€ë¶„ë¥˜ì˜ ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ ë°˜ë³µ
        print(f"[{main_category}] {sub_category} ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...") # ì§„í–‰ ìƒí™© ì¶œë ¥
        articles = fetch_news(sub_category, main_category) # ë‰´ìŠ¤ ìˆ˜ì§‘ í•¨ìˆ˜ í˜¸ì¶œ

        for article in articles:                   # ìˆ˜ì§‘ëœ ê° ê¸°ì‚¬ì— ëŒ€í•´ ë°˜ë³µ
            article['evaluation'] = gpt_evaluate(article['content'], user_selected_sources) # GPT í‰ê°€ í•¨ìˆ˜ í˜¸ì¶œ ë° ê²°ê³¼ ì €ì¥
            time.sleep(1)                          # API í˜¸ì¶œ ê°„ 1ì´ˆ ì§€ì—° (ì„œë²„ ë¶€ë‹´ ì™„í™”)

        save_news(main_category, sub_category, articles) # HTML íŒŒì¼ë¡œ ì €ì¥
        print(f" Â -> {len(articles)}ê°œ ë‰´ìŠ¤ ì €ì¥ ì™„ë£Œ") # ì €ì¥ ì™„ë£Œ ë©”ì‹œì§€ ì¶œë ¥

print("\nğŸ‰ ëª¨ë“  ë‰´ìŠ¤ ìˆ˜ì§‘ ë° GPT í‰ê°€ ì™„ë£Œ!") # ìµœì¢… ì™„ë£Œ ë©”ì‹œì§€ ì¶œë ¥
