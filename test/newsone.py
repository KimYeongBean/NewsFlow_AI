# ================================
# 0. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ================================
import feedparser
import time
import os
from datetime import datetime, timedelta
from urllib.parse import quote
from openai import AzureOpenAI
import re
import requests
import uuid
from bs4 import BeautifulSoup
import asyncio
from playwright.async_api import async_playwright

# ================================
# 1. ì‚¬ìš©ì ì„¤ì •
# ================================
user_selected_sources = ["ì¡°ì„ ì¼ë³´", "í•œê²¨ë ˆ", "ì¤‘ì•™ì¼ë³´", "ë™ì•„ì¼ë³´", "ê²½í–¥ì‹ ë¬¸"]
user_follow_categories = ["ì—¬í–‰"]

# ================================
# 2. Azure AI ì„œë¹„ìŠ¤ ì„¤ì • (ì›ë³¸ ê°’ìœ¼ë¡œ ë³µì›)
# ================================
# --- Azure AI ë²ˆì—­ê¸°(Translator) ì„¤ì • ---
translator_key = "5NuWjUHv52i3letxBdeZw1V46HADYfjoUdUc8aJqBm38uBSl16u4JQQJ99BHACNns7RXJ3w3AAAbACOG8bu6"
translator_endpoint = "https://api.cognitive.microsofttranslator.com/"
translator_location = "KoreaCentral"

# ================================
# 3. Azure OpenAI ì´ˆê¸°í™” (ì›ë³¸ ê°’ìœ¼ë¡œ ë³µì›)
# ================================
endpoint = "https://newscheck2.openai.azure.com/"
deployment = "gpt-4o"
subscription_key = "Dsf5DmuTn1cS7lXaSxSTnO30kTZCqr2xKqIjLwvdovEGnQsz3NjlJQQJ99BHACHYHv6XJ3w3AAABACOGJk53"
api_version = "2025-01-01-preview"

client = AzureOpenAI(
    azure_endpoint=endpoint,
    api_key=subscription_key,
    api_version=api_version,
)

# ================================
# 4. ì „ì²´ ë‰´ìŠ¤/ì¹´í…Œê³ ë¦¬
# ================================
all_sources = [
    'MBCë‰´ìŠ¤', 'ì—°í•©ë‰´ìŠ¤', 'ì¡°ì„ ì¼ë³´', 'ë‰´ìŠ¤1', 'JTBC ë‰´ìŠ¤',
    'ì¤‘ì•™ì¼ë³´', 'SBS ë‰´ìŠ¤', 'YTN', 'í•œê²¨ë ˆ', 'ê²½í–¥ì‹ ë¬¸',
    'ì˜¤ë§ˆì´ë‰´ìŠ¤', 'í•œêµ­ê²½ì œ'
]
categories = {
    'ì •ì¹˜': ['ëŒ€í†µë ¹ì‹¤', 'êµ­íšŒ', 'ì •ë‹¹', 'í–‰ì •', 'ì™¸êµ', 'êµ­ë°©/ë¶í•œ'],
    'ê²½ì œ': ['ê¸ˆìœµ/ì¦ê¶Œ', 'ì‚°ì—…/ì¬ê³„', 'ì¤‘ê¸°/ë²¤ì²˜', 'ë¶€ë™ì‚°', 'ê¸€ë¡œë²Œ', 'ìƒí™œ'],
    'ì‚¬íšŒ': ['ì‚¬ê±´ì‚¬ê³ ', 'êµìœ¡', 'ë…¸ë™', 'ì–¸ë¡ ', 'í™˜ê²½', 'ì¸ê¶Œ/ë³µì§€', 'ì‹í’ˆ/ì˜ë£Œ', 'ì§€ì—­', 'ì¸ë¬¼'],
    'IT_ê³¼í•™': ['ëª¨ë°”ì¼', 'ì¸í„°ë„·/SNS', 'í†µì‹ /ë‰´ë¯¸ë””ì–´', 'IT', 'ë³´ì•ˆ/í•´í‚¹', 'ì»´í“¨í„°', 'ê²Œì„/ë¦¬ë·°', 'ê³¼í•™'],
    'ìƒí™œ_ë¬¸í™”': ['ê±´ê°•', 'ìë™ì°¨', 'ì—¬í–‰/ë ˆì €', 'ìŒì‹/ë§›ì§‘', 'íŒ¨ì…˜/ë·°í‹°', 'ê³µì—°/ì „ì‹œ', 'ì±…', 'ì¢…êµ', 'ë‚ ì”¨', 'ìƒí™œ'],
    'ì„¸ê³„': ['ì•„ì‹œì•„/í˜¸ì£¼', 'ë¯¸êµ­/ì¤‘ë‚¨ë¯¸', 'ìœ ëŸ½', 'ì¤‘ë™/ì•„í”„ë¦¬ì¹´', 'ì„¸ê³„'],
    'ì—¬í–‰': ['êµ­ë‚´ ì—¬í–‰']
}
MAX_ARTICLES_PER_CATEGORY = 100
save_path = 'C:/Users/admin/Desktop/news/test1/output'
one_month_ago = datetime.now() - timedelta(days=30)
os.makedirs(save_path, exist_ok=True)

# ================================
# 4-1. UI ë²ˆì—­ í…ìŠ¤íŠ¸
# ================================
ui_texts = {
    'ko': {'reliability_label': 'ì‹ ë¢°ë„', 'high': 'ë†’ìŒ', 'medium': 'ë³´í†µ', 'low': 'ë‚®ìŒ'},
    'en': {'reliability_label': 'Reliability', 'high': 'High', 'medium': 'Medium', 'low': 'Low'},
    'ja': {'reliability_label': 'ä¿¡é ¼åº¦', 'high': 'é«˜ã„', 'medium': 'æ™®é€š', 'low': 'ä½ã„'},
    'fr': {'reliability_label': 'FiabilitÃ©', 'high': 'Ã‰levÃ©e', 'medium': 'Moyenne', 'low': 'Faible'},
    'zh-Hans': {'reliability_label': 'å¯ä¿¡åº¦', 'high': 'é«˜', 'medium': 'ä¸­', 'low': 'ä½'}
}

# ====================================================================
# 5. ì›ë¬¸/ì´ë¯¸ì§€ ì£¼ì†Œ ì¶”ì¶œ í•¨ìˆ˜ (Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ JS ë¦¬ë‹¤ì´ë ‰íŠ¸ ì™„ë²½ í•´ê²°)
# ====================================================================
async def get_original_article_info_async(google_news_url):
    print(f"  -> [ì£¼ì†Œ ë³€í™˜ ì‹œë„] êµ¬ê¸€ ë‰´ìŠ¤ URL: {google_news_url}")
    original_url = google_news_url
    image_url = None
    
    try:
        async with async_playwright() as p:
            # Headless Chromium ë¸Œë¼ìš°ì € ì‹¤í–‰ (ì„œë²„ í™˜ê²½ì„ ìœ„í•´ --no-sandbox ì˜µì…˜ ì¶”ê°€)
            browser = await p.chromium.launch(headless=True, args=['--no-sandbox'])
            page = await browser.new_page()
            
            # êµ¬ê¸€ ë‰´ìŠ¤ URLë¡œ ì´ë™ (30ì´ˆ íƒ€ì„ì•„ì›ƒ)
            await page.goto(google_news_url, timeout=30000)
            
            # ìë°”ìŠ¤í¬ë¦½íŠ¸ ë¦¬ë‹¤ì´ë ‰ì…˜ì´ ì™„ë£Œë  ì‹œê°„ì„ ë²Œì–´ì£¼ê¸° ìœ„í•´ ì ì‹œ ëŒ€ê¸°
            await page.wait_for_timeout(3000)
            
            # ìµœì¢…ì ìœ¼ë¡œ ì •ì°©í•œ í˜ì´ì§€ì˜ URLê³¼ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°
            original_url = page.url
            content = await page.content()
            
            print(f"  -> [ë³€í™˜ ì™„ë£Œ] ì›ë¬¸ ì£¼ì†Œ: {original_url}")
            
            # ì´ë¯¸ì§€ íƒœê·¸ ê²€ìƒ‰
            soup = BeautifulSoup(content, 'html.parser')
            image_tag = soup.find('meta', property='og:image')
            
            if image_tag and image_tag.get('content'):
                image_url = image_tag['content']
                print("  -> ì´ë¯¸ì§€ ì£¼ì†Œ ì°¾ìŒ!")
            else:
                print("  [ì•Œë¦¼] ì´ í˜ì´ì§€ì—ëŠ” og:image íƒœê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            await browser.close()

    except Exception as e:
        print(f"  [ì˜¤ë¥˜] Playwright ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
    return {'original_url': original_url, 'image_url': image_url}

# ê¸°ì¡´ ë™ê¸°ì‹ ì½”ë“œì™€ í˜¸í™˜ì„ ìœ„í•œ ë˜í¼(wrapper) í•¨ìˆ˜
def get_original_article_info(google_news_url):
    # ì´ë²¤íŠ¸ ë£¨í”„ë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒˆë¡œ ìƒì„±í•˜ì—¬ ì‹¤í–‰
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    
    return loop.run_until_complete(get_original_article_info_async(google_news_url))


# ================================
# 5-1. ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ í•¨ìˆ˜
# ================================
def scrape_article_body(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        article_tag = soup.find('article')
        if not article_tag:
            selectors = ['#articleBodyContents', '#article_body', '.article_body', '#dic_area']
            for selector in selectors:
                article_tag = soup.select_one(selector)
                if article_tag: break
        if article_tag:
            paragraphs = article_tag.find_all('p')
            if not paragraphs: paragraphs = article_tag.find_all('div')
            body_text = "\n".join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
            if len(body_text) > 50:
                print("  -> ë³¸ë¬¸ ìˆ˜ì§‘ ì„±ê³µ!")
                return body_text
        print("  [ì•Œë¦¼] ê¸°ì‚¬ ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.")
        return None
    except Exception as e:
        print(f"  [ì˜¤ë¥˜] ë³¸ë¬¸ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# ================================
# 6. ë‰´ìŠ¤ ìˆ˜ì§‘ ë° GPT í‰ê°€ í•¨ìˆ˜
# ================================
def fetch_news(sub_category):
    encoded_keyword = quote(sub_category)
    news_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl=ko&gl=KR"
    feed = feedparser.parse(news_url)
    articles = []
    for entry in feed.entries:
        if len(articles) >= MAX_ARTICLES_PER_CATEGORY: break
        source = getattr(entry, 'source', None)
        if source and source.title in all_sources:
            published_time = entry.get('published_parsed')
            if not published_time: continue
            article_date = datetime.fromtimestamp(time.mktime(published_time))
            if article_date < one_month_ago: continue
            
            print(f"Processing '{entry.title[:30]}...'")
            article_info = get_original_article_info(entry.link)
            body_text = None
            if "news.google.com" not in article_info['original_url']:
                body_text = scrape_article_body(article_info['original_url'])
            
            articles.append({
                'title': entry.title,
                'link': article_info['original_url'],
                'image_url': article_info['image_url'],
                'source': source.title,
                'date': article_date.strftime('%Y-%m-%d %H:%M:%S'),
                'content': body_text if body_text else entry.title
            })
    return articles

def gpt_evaluate(article_text, selected_sources):
    prompt_text = f"ë‹¹ì‹ ì€ ë‰´ìŠ¤ ìš”ì•½ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.\nì‚¬ìš©ìê°€ ì„ íƒí•œ ì–¸ë¡ ì‚¬: {', '.join(selected_sources)}\n\nì•„ë˜ ë‰´ìŠ¤ ì œëª© ë˜ëŠ” ë³¸ë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ:\n1) 3ì¤„ë¡œ ìš”ì•½\n2) ì„ íƒí•œ ì–¸ë¡ ì‚¬ì™€ í•µì‹¬ ì£¼ì¥ ë¹„êµ\n3) ì‹ ë¢°ë„ ë“±ê¸‰ ì¶œë ¥ (ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ë§Œ ì‚¬ìš©):\n    ì‹ ë¢°ë„: ë†’ìŒ / ë³´í†µ / ë‚®ìŒ\n\nì‹ ë¢°ë„ í‰ê°€ ê¸°ì¤€:\n- ì£¼ìš” ì–¸ë¡ ì‚¬(ì¡°ì„ ì¼ë³´, í•œê²¨ë ˆ, ì¤‘ì•™ì¼ë³´, ë™ì•„ì¼ë³´, ê²½í–¥ì‹ ë¬¸) â†’ ë†’ìŒ\n- ì œëª©ë§Œ ì¡´ì¬í•˜ê±°ë‚˜ ì¼ë¶€ ì •ë³´ë§Œ ìˆëŠ” ê²½ìš° â†’ ë³´í†µ\n- ê·¼ê±° ë¶€ì¡±/ì„ ì •ì /ì¶œì²˜ ë¶ˆë¶„ëª… â†’ ë‚®ìŒ\n\nâš ï¸ ì¶œë ¥ í˜•ì‹ì„ ë°˜ë“œì‹œ ì§€ì¼œì£¼ì„¸ìš”. ë‹¤ë¥¸ ë§ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”."
    messages = [{"role": "system", "content": "ë„ˆëŠ” ë‰´ìŠ¤ ìš”ì•½ê³¼ ì–¸ë¡ ì‚¬ ë¹„êµ, ì‹ ë¢°ë„ í‰ê°€ë§Œ ê°„ë‹¨íˆ ì¶œë ¥í•˜ëŠ” ë„ìš°ë¯¸ì•¼."}, {"role": "user", "content": prompt_text}, {"role": "user", "content": article_text}]
    try:
        completion = client.chat.completions.create(model=deployment, messages=messages, max_tokens=1024)
        return completion.choices[0].message.content.strip()
    except Exception as e:
        return f"GPT í‰ê°€ ì˜¤ë¥˜: {e}"

# ================================
# 7. Azure ë²ˆì—­ í•¨ìˆ˜
# ================================
def translate_with_azure(text_to_translate, target_languages):
    headers = {'Ocp-Apim-Subscription-Key': translator_key, 'Ocp-Apim-Subscription-Region': translator_location, 'Content-type': 'application/json', 'X-ClientTraceId': str(uuid.uuid4())}
    params = {'api-version': '3.0', 'from': 'ko', 'to': target_languages}
    body = [{'text': text_to_translate}]
    try:
        response = requests.post(f"{translator_endpoint}/translate", params=params, headers=headers, json=body)
        response.raise_for_status()
        translations = response.json()
        return {t['to']: t['text'] for t in translations[0]['translations']}
    except Exception as e:
        print(f"Azure ë²ˆì—­ API ì˜¤ë¥˜: {e}")
        return {lang: "ë²ˆì—­ ì˜¤ë¥˜" for lang in target_languages}

# ================================
# 8. HTML ì €ì¥ í•¨ìˆ˜ (ìˆ˜ì •ë¨)
# ================================
def save_news_with_translations(main_category, sub_category, articles):
    main_path = os.path.join(save_path, main_category)
    os.makedirs(main_path, exist_ok=True)
    file_name = f"{sub_category.replace('/', '_')}_news.html"
    full_path = os.path.join(main_path, file_name)
    with open(full_path, 'w', encoding='utf-8') as f:
        f.write("<html><head><meta charset='utf-8'><title>ë‰´ìŠ¤ ìš”ì•½</title><style>body{font-family:'Malgun Gothic',sans-serif;margin:20px;line-height:1.6}.article-block{border-bottom:1px solid #ddd;padding-bottom:15px;margin-bottom:15px}.lang-buttons{margin-bottom:10px}.lang-buttons button{padding:8px 12px;cursor:pointer;border:1px solid #ccc;background-color:#f0f0f0;margin-right:5px;border-radius:5px}.lang-buttons button.active{background-color:#3498db;color:white;border-color:#3498db}.content-wrapper .content{display:none}.content-wrapper .content.active{display:block}.summary{background-color:#f8f9f9;border-left:4px solid #3498db;padding:10px;margin-top:5px}.reliability{font-weight:bold;padding:3px 8px;border-radius:5px;color:white;display:inline-block;margin-left:10px}.high{background-color:#2ecc71}.medium{background-color:#f39c12}.low{background-color:#e74c3c}.article-image{max-width:100%;height:auto;border-radius:8px;margin-bottom:10px;object-fit:cover;max-height:400px}.no-image-text{color:#888;font-style:italic;margin-bottom:10px;display:block}</style></head><body>")
        f.write(f"<h1>{main_category} - {sub_category} ë‰´ìŠ¤</h1>")
        f.write('<div class="lang-buttons"><strong>ì „ì²´ ì–¸ì–´ ë³€ê²½:</strong><button onclick="changeAllLanguages(\'ko\')" class="active">í•œêµ­ì–´</button><button onclick="changeAllLanguages(\'en\')">English</button><button onclick="changeAllLanguages(\'ja\')">æ—¥æœ¬èª</button><button onclick="changeAllLanguages(\'zh-Hans\')">ä¸­æ–‡(ç®€ä½“)</button><button onclick="changeAllLanguages(\'fr\')">FranÃ§ais</button></div><hr>')
        
        for i, article in enumerate(articles):
            f.write(f'<div class="article-block" id="article-{i}">')
            if article.get('image_url'):
                f.write(f"<img src='{article['image_url']}' alt='ê¸°ì‚¬ ëŒ€í‘œ ì´ë¯¸ì§€' class='article-image' onerror='this.style.display=\"none\"'>")
            else:
                f.write("<span class='no-image-text'>[ì´ë¯¸ì§€ ì—†ìŒ]</span>")
            
            f.write(f"<p><b>ì–¸ë¡ ì‚¬:</b> {article['source']} | <b>ë°œí–‰ ì‹œê°„:</b> {article['date']}</p><div class='content-wrapper'>")
            
            for lang, content in article['translations'].items():
                active_class = "active" if lang == 'ko' else ""
                title = content['title']
                summary_html = content['summary_html']
                
                f.write(f'<div class="content {lang} {active_class}">')
                f.write(f'<h3><a href=\'{article["link"]}\' target=\'blank\'>{title}</a></h3>')
                f.write(summary_html)
                f.write('</div>')
            
            f.write('</div></div>')
        f.write('<script>function changeAllLanguages(lang){document.querySelectorAll(".lang-buttons button").forEach(b=>b.classList.remove("active"));document.querySelector(`.lang-buttons button[onclick="changeAllLanguages(\'${lang}\')"]`).classList.add("active");document.querySelectorAll(".article-block").forEach(article=>{article.querySelectorAll(".content").forEach(c=>c.classList.remove("active"));const target=article.querySelector(`.content.${lang}`);if(target)target.classList.add("active")})}</script></body></html>')

# ================================
# 9. ë©”ì¸ ì‹¤í–‰ ë£¨í”„ (ìµœì¢… ìˆ˜ì •)
# ================================
target_languages = ['en', 'ja', 'fr', 'zh-Hans']

for main_category, sub_categories in categories.items():
    if main_category not in user_follow_categories: continue
    for sub_category in sub_categories:
        print(f"[{main_category}] {sub_category} ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        articles = fetch_news(sub_category)
        if not articles:
            print("  -> ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            continue
        
        processed_articles = []
        for article in articles:
            print(f"  - '{article['title'][:30]}...' GPT í‰ê°€ ë° ë²ˆì—­ ì¤‘...")

            evaluation_text = gpt_evaluate(article['content'], user_selected_sources)
            
            time.sleep(1)
            
            summary_match = re.search(r'1\)(.*?)(?=2\)|\Z)', evaluation_text, re.DOTALL)
            reliability_match = re.search(r'ì‹ ë¢°ë„:\s*(ë†’ìŒ|ë³´í†µ|ë‚®ìŒ)', evaluation_text)
            
            summary_text = summary_match.group(1).strip() if summary_match else "ìš”ì•½ ì •ë³´ ì—†ìŒ"
            reliability = reliability_match.group(1).strip() if reliability_match else "ì•Œ ìˆ˜ ì—†ìŒ"
            
            # 1. ë²ˆì—­í•  í…ìŠ¤íŠ¸ ì¤€ë¹„ ë° API í˜¸ì¶œ
            text_to_translate = f"{article['title']}\n{summary_text}"
            azure_translations = translate_with_azure(text_to_translate, target_languages)

            # 2. ëª¨ë“  ì–¸ì–´ì˜ ì œëª©ê³¼ ìš”ì•½ ë‚´ìš©ì„ ë‹´ì„ ë”•ì…”ë„ˆë¦¬ ì¤€ë¹„
            all_content = {'ko': {'title': article['title'], 'summary': summary_text}}
            for lang, translated_full_text in azure_translations.items():
                parts = translated_full_text.split('\n', 1)
                all_content[lang] = {
                    'title': parts[0],
                    'summary': parts[1] if len(parts) > 1 else ""
                }
            
            # 3. ì‹ ë¢°ë„ ê°’ì— ëŒ€í•œ í‚¤ì™€ CSS í´ë˜ìŠ¤ ì„¤ì •
            reliability_key_map = {"ë†’ìŒ": "high", "ë³´í†µ": "medium", "ë‚®ìŒ": "low"}
            reliability_key = reliability_key_map.get(reliability, "low")
            reliability_class = reliability_key
            
            # 4. ê° ì–¸ì–´ë³„ë¡œ ë²ˆì—­ëœ UI í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ HTML ìƒì„±
            final_translations = {}
            for lang, content in all_content.items():
                # ui_texts ë”•ì…”ë„ˆë¦¬ì—ì„œ í˜„ì¬ ì–¸ì–´ì— ë§ëŠ” í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
                reliability_label = ui_texts[lang]['reliability_label']
                translated_reliability_value = ui_texts[lang][reliability_key]
                
                # ë²ˆì—­ëœ í…ìŠ¤íŠ¸ë¡œ summary_html ìƒì„±
                summary_html = (f"<div class='summary'>{content['summary'].replace('\n', '<br>')}<br>"
                                f"<span class='reliability {reliability_class}'>"
                                f"{reliability_label}: {translated_reliability_value}</span></div>")
                
                final_translations[lang] = {
                    'title': content['title'],
                    'summary_html': summary_html
                }

            article_data = {
                'link': article['link'], 
                'image_url': article['image_url'], 
                'source': article['source'], 
                'date': article['date'],
                'translations': final_translations
            }
            
            processed_articles.append(article_data)
        
        save_news_with_translations(main_category, sub_category, processed_articles)
        print(f"  -> {len(processed_articles)}ê°œ ë‰´ìŠ¤ ì €ì¥ ì™„ë£Œ.")

print("\nğŸ‰ ëª¨ë“  ë‰´ìŠ¤ ìˆ˜ì§‘, í‰ê°€ ë° ë²ˆì—­ ì™„ë£Œ!")