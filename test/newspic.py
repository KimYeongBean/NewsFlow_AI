# ================================
# 0. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ================================
import feedparser
import time
import os
from datetime import datetime, timedelta
from urllib.parse import quote
from openai import AzureOpenAI
import re
import requests
import uuid
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait

# ================================
# 1. ì‚¬ìš©ì ì„¤ì •
# ================================
user_selected_sources = ["ì¡°ì„ ì¼ë³´", "í•œê²¨ë ˆ", "ì¤‘ì•™ì¼ë³´", "ë™ì•„ì¼ë³´", "ê²½í–¥ì‹ ë¬¸"]
user_follow_categories = ["ì—¬í–‰"]

# ================================
# 2. Azure AI ì„œë¹„ìŠ¤ ì„¤ì •
# ================================
# --- Azure AI ë²ˆì—­ê¸°(Translator) ì„¤ì • ---
translator_key = "5NuWjUHv52i3letxBdeZw1V46HADYfjoUdUc8aJqBm38uBSl16u4JQQJ99BHACNns7RXJ3w3AAAbACOG8bu6"
translator_endpoint = "https://api.cognitive.microsofttranslator.com/"
translator_location = "KoreaCentral"

# ================================
# 3. Azure OpenAI ì´ˆê¸°í™”
# ================================
endpoint = "https://newscheck2.openai.azure.com/"
deployment = "gpt-5-nano"
subscription_key = "Dsf5DmuTn1cS7lXaSxSTnO30kTZCqr2xKqIjLwvdovEGnQsz3NjlJQQJ99BHACHYHv6XJ3w3AAABACOGJk53"

client = AzureOpenAI(
    azure_endpoint=endpoint,
    api_key=subscription_key,
    api_version="2025-01-01-preview",
)

# ================================
# 4. ì „ì²´ ë‰´ìŠ¤/ì¹´í…Œê³ ë¦¬
# ================================
all_sources = [
    'MBCë‰´ìŠ¤', 'ì—°í•©ë‰´ìŠ¤', 'ì¡°ì„ ì¼ë³´', 'ë‰´ìŠ¤1', 'JTBC ë‰´ìŠ¤',
    'ì¤‘ì•™ì¼ë³´', 'SBS ë‰´ìŠ¤', 'YTN', 'í•œê²¨ë ˆ', 'ê²½í–¥ì‹ ë¬¸',
    'ì˜¤ë§ˆì´ë‰´ìŠ¤', 'í•œêµ­ê²½ì œ'
]
categories = {
    'ì •ì¹˜': ['ëŒ€í†µë ¹ì‹¤', 'êµ­íšŒ', 'ì •ë‹¹', 'í–‰ì •', 'ì™¸êµ', 'êµ­ë°©/ë¶í•œ'],
    'ê²½ì œ': ['ê¸ˆìœµ/ì¦ê¶Œ', 'ì‚°ì—…/ì¬ê³„', 'ì¤‘ê¸°/ë²¤ì²˜', 'ë¶€ë™ì‚°', 'ê¸€ë¡œë²Œ', 'ìƒí™œ'],
    'ì‚¬íšŒ': ['ì‚¬ê±´ì‚¬ê³ ', 'êµìœ¡', 'ë…¸ë™', 'ì–¸ë¡ ', 'í™˜ê²½', 'ì¸ê¶Œ/ë³µì§€', 'ì‹í’ˆ/ì˜ë£Œ', 'ì§€ì—­', 'ì¸ë¬¼'],
    'IT_ê³¼í•™': ['ëª¨ë°”ì¼', 'ì¸í„°ë„·/SNS', 'í†µì‹ /ë‰´ë¯¸ë””ì–´', 'IT', 'ë³´ì•ˆ/í•´í‚¹', 'ì»´í“¨í„°', 'ê²Œì„/ë¦¬ë·°', 'ê³¼í•™'],
    'ìƒí™œ_ë¬¸í™”': ['ê±´ê°•', 'ìë™ì°¨', 'ì—¬í–‰/ë ˆì €', 'ìŒì‹/ë§›ì§‘', 'íŒ¨ì…˜/ë·°í‹°', 'ê³µì—°/ì „ì‹œ', 'ì±…', 'ì¢…êµ', 'ë‚ ì”¨', 'ìƒí™œ'],
    'ì„¸ê³„': ['ì•„ì‹œì•„/í˜¸ì£¼', 'ë¯¸êµ­/ì¤‘ë‚¨ë¯¸', 'ìœ ëŸ½', 'ì¤‘ë™/ì•„í”„ë¦¬ì¹´', 'ì„¸ê³„'],
    'ì—¬í–‰': ['êµ­ë‚´ ì—¬í–‰']
}
MAX_ARTICLES_PER_CATEGORY = 100
save_path = 'C:/Users/admin/Desktop/news/test1/output'
one_month_ago = datetime.now() - timedelta(days=30)
os.makedirs(save_path, exist_ok=True)

# ================================
# 5. ì›ë¬¸/ì´ë¯¸ì§€ ì£¼ì†Œ ì¶”ì¶œ í•¨ìˆ˜ (WebDriverWait ì ìš©)
# ================================
def get_original_article_info(google_news_url):
    """
    Seleniumì˜ WebDriverWaitë¥¼ ì´ìš©í•´ URLì´ ì‹¤ì œë¡œ ë³€ê²½ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦° í›„
    ì›ë¬¸ ì£¼ì†Œì™€ ì´ë¯¸ì§€ ì£¼ì†Œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    print(f"  -> [ë³€í™˜ ì‹œë„] ê¸°ì¡´ ì£¼ì†Œ: {google_news_url}")

    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")

    service = Service()
    driver = webdriver.Chrome(service=service, options=chrome_options)
    
    original_url = google_news_url
    image_url = None

    try:
        driver.get(google_news_url)
        
        wait = WebDriverWait(driver, 15)
        wait.until(lambda driver: "news.google.com" not in driver.current_url)
        
        original_url = driver.current_url
        print(f"  -> [ë³€í™˜ ì™„ë£Œ] ì›ë¬¸ ì£¼ì†Œ: {original_url}")
        
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        image_tag = soup.find('meta', property='og:image')
        
        if image_tag and image_tag.get('content'):
            image_url = image_tag['content']
            print("  -> ì´ë¯¸ì§€ ì£¼ì†Œ ì°¾ìŒ!")
        else:
            print("  [ì•Œë¦¼] ì´ í˜ì´ì§€ì—ëŠ” og:image íƒœê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"  [ì˜¤ë¥˜] ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ (íƒ€ì„ì•„ì›ƒ ë˜ëŠ” ê¸°íƒ€): {e}")
        original_url = driver.current_url
        print(f"  -> [ì˜¤ë¥˜ ì‹œì  ì£¼ì†Œ] {original_url}")
    finally:
        driver.quit()
        
    return {'original_url': original_url, 'image_url': image_url}

# ================================
# 6. ë‰´ìŠ¤ ìˆ˜ì§‘ ë° GPT í‰ê°€ í•¨ìˆ˜
# ================================
def fetch_news(sub_category):
    encoded_keyword = quote(sub_category)
    news_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl=ko&gl=KR"
    feed = feedparser.parse(news_url)
    articles = []
    for entry in feed.entries:
        if len(articles) >= MAX_ARTICLES_PER_CATEGORY: break
        source = getattr(entry, 'source', None)
        if source and source.title in all_sources:
            published_time = entry.get('published_parsed')
            if not published_time: continue
            article_date = datetime.fromtimestamp(time.mktime(published_time))
            if article_date < one_month_ago: continue
            
            print(f"Processing '{entry.title[:30]}...'")
            article_info = get_original_article_info(entry.link)

            articles.append({
                'title': entry.title,
                'link': article_info['original_url'],
                'image_url': article_info['image_url'],
                'source': source.title,
                'date': article_date.strftime('%Y-%m-%d %H:%M:%S'),
                'content': entry.title
            })
    return articles

def gpt_evaluate(article_text, selected_sources):
    prompt_text = f"""
ë‹¹ì‹ ì€ ë‰´ìŠ¤ ìš”ì•½ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìê°€ ì„ íƒí•œ ì–¸ë¡ ì‚¬: {', '.join(selected_sources)}

ì•„ë˜ ë‰´ìŠ¤ ì œëª© ë˜ëŠ” ë³¸ë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ:
1) 3ì¤„ë¡œ ìš”ì•½
2) ì„ íƒí•œ ì–¸ë¡ ì‚¬ì™€ í•µì‹¬ ì£¼ì¥ ë¹„êµ
3) ì‹ ë¢°ë„ ë“±ê¸‰ ì¶œë ¥ (ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ë§Œ ì‚¬ìš©):
    ì‹ ë¢°ë„: ë†’ìŒ / ë³´í†µ / ë‚®ìŒ

ì‹ ë¢°ë„ í‰ê°€ ê¸°ì¤€:
- ì£¼ìš” ì–¸ë¡ ì‚¬(ì¡°ì„ ì¼ë³´, í•œê²¨ë ˆ, ì¤‘ì•™ì¼ë³´, ë™ì•„ì¼ë³´, ê²½í–¥ì‹ ë¬¸) â†’ ë†’ìŒ
- ì œëª©ë§Œ ì¡´ì¬í•˜ê±°ë‚˜ ì¼ë¶€ ì •ë³´ë§Œ ìˆëŠ” ê²½ìš° â†’ ë³´í†µ
- ê·¼ê±° ë¶€ì¡±/ì„ ì •ì /ì¶œì²˜ ë¶ˆë¶„ëª… â†’ ë‚®ìŒ

âš ï¸ ì¶œë ¥ í˜•ì‹ì„ ë°˜ë“œì‹œ ì§€ì¼œì£¼ì„¸ìš”.
"""
    messages = [{"role": "system", "content": "ë„ˆëŠ” ë‰´ìŠ¤ ìš”ì•½ê³¼ ì–¸ë¡ ì‚¬ ë¹„êµ, ì‹ ë¢°ë„ í‰ê°€ë§Œ ê°„ë‹¨íˆ ì¶œë ¥í•˜ëŠ” ë„ìš°ë¯¸ì•¼."}, {"role": "user", "content": prompt_text}, {"role": "user", "content": article_text}]
    try:
        completion = client.chat.completions.create(model=deployment, messages=messages, max_tokens=1024)
        return completion.choices[0].message.content.strip()
    except Exception as e:
        return f"GPT í‰ê°€ ì˜¤ë¥˜: {e}"

# ================================
# 7. Azure ë²ˆì—­ í•¨ìˆ˜
# ================================
def translate_with_azure(text_to_translate, target_languages):
    headers = {
        'Ocp-Apim-Subscription-Key': translator_key,
        'Ocp-Apim-Subscription-Region': translator_location,
        'Content-type': 'application/json',
        'X-ClientTraceId': str(uuid.uuid4())
    }
    params = { 'api-version': '3.0', 'from': 'ko', 'to': target_languages }
    body = [{'text': text_to_translate}]
    
    try:
        response = requests.post(f"{translator_endpoint}/translate", params=params, headers=headers, json=body)
        response.raise_for_status()
        translations = response.json()
        return {t['to']: t['text'] for t in translations[0]['translations']}
    except requests.exceptions.RequestException as e:
        print(f"Azure ë²ˆì—­ API ì˜¤ë¥˜: {e}")
        return {lang: "ë²ˆì—­ ì˜¤ë¥˜" for lang in target_languages}

# ================================
# 8. HTML ì €ì¥ í•¨ìˆ˜
# ================================
def save_news_with_translations(main_category, sub_category, articles):
    main_path = os.path.join(save_path, main_category)
    os.makedirs(main_path, exist_ok=True)
    file_name = f"{sub_category.replace('/', '_')}_news.html"
    full_path = os.path.join(main_path, file_name)

    with open(full_path, 'w', encoding='utf-8') as f:
        f.write("<html><head><meta charset='utf-8'><title>ë‰´ìŠ¤ ìš”ì•½</title>")
        f.write("""
        <style>
            body { font-family: 'Malgun Gothic', sans-serif; margin: 20px; line-height: 1.6; }
            .article-block { border-bottom: 1px solid #ddd; padding-bottom: 15px; margin-bottom: 15px; }
            .lang-buttons { margin-bottom: 10px; }
            .lang-buttons button { padding: 8px 12px; cursor: pointer; border: 1px solid #ccc; background-color: #f0f0f0; margin-right: 5px; border-radius: 5px; }
            .lang-buttons button.active { background-color: #3498db; color: white; border-color: #3498db; }
            .content-wrapper .content { display: none; }
            .content-wrapper .content.active { display: block; }
            .summary { background-color: #f8f9f9; border-left: 4px solid #3498db; padding: 10px; margin-top: 5px; }
            .reliability { font-weight: bold; padding: 3px 8px; border-radius: 5px; color: white; display: inline-block; margin-left: 10px; }
            .high { background-color: #2ecc71; } .medium { background-color: #f39c12; } .low { background-color: #e74c3c; }
            .article-image {
                max-width: 100%; height: auto; border-radius: 8px; margin-bottom: 10px; object-fit: cover; max-height: 400px;
            }
            .no-image-text {
                color: #888; font-style: italic; margin-bottom: 10px; display: block;
            }
        </style>
        </head><body>""")
        f.write(f"<h1>{main_category} - {sub_category} ë‰´ìŠ¤</h1>")

        f.write('<div class="lang-buttons"><strong>ì „ì²´ ì–¸ì–´ ë³€ê²½:</strong>')
        f.write('<button onclick="changeAllLanguages(\'ko\')" class="active">í•œêµ­ì–´</button>')
        f.write('<button onclick="changeAllLanguages(\'en\')">English</button>')
        f.write('<button onclick="changeAllLanguages(\'ja\')">æ—¥æœ¬èª</button>')
        f.write('<button onclick="changeAllLanguages(\'zh-Hans\')">ä¸­æ–‡(ç®€ä½“)</button>')
        f.write('<button onclick="changeAllLanguages(\'fr\')">FranÃ§ais</button>')
        f.write('</div><hr>')

        for i, article in enumerate(articles):
            f.write(f'<div class="article-block" id="article-{i}">')
            if article.get('image_url'):
                f.write(f"<img src='{article['image_url']}' alt='ê¸°ì‚¬ ëŒ€í‘œ ì´ë¯¸ì§€' class='article-image' onerror='this.style.display=\"none\"'>")
            else:
                f.write("<span class='no-image-text'>[ì´ë¯¸ì§€ ì—†ìŒ]</span>")
                
            f.write(f"<p><b>ì–¸ë¡ ì‚¬:</b> {article['source']} | <b>ë°œí–‰ ì‹œê°„:</b> {article['date']}</p>")
            f.write('<div class="content-wrapper">')
            for lang, content in article['translations'].items():
                active_class = "active" if lang == 'ko' else ""
                if lang == 'ko':
                    title, summary_html = content['title'], content['summary_html']
                else:
                    title, summary_html = content, article['translations']['ko']['summary_html']
                f.write(f'<div class="content {lang} {active_class}">')
                f.write(f"<h3><a href='{article['link']}' target='blank'>{title}</a></h3>")
                if lang == 'ko' and summary_html:
                    f.write(summary_html)
                f.write('</div>')
            f.write('</div></div>')

        f.write('''
        <script>
            function changeAllLanguages(lang) {
                document.querySelectorAll('.lang-buttons button').forEach(b => b.classList.remove('active'));
                document.querySelector(`.lang-buttons button[onclick="changeAllLanguages('${lang}')"]`).classList.add('active');
                document.querySelectorAll('.article-block').forEach(article => {
                    article.querySelectorAll('.content').forEach(c => c.classList.remove('active'));
                    const target = article.querySelector(`.content.${lang}`);
                    if (target) target.classList.add('active');
                });
            }
        </script>''')
        f.write("</body></html>")

# ================================
# 9. ë©”ì¸ ì‹¤í–‰ ë£¨í”„
# ================================
target_languages = ['en', 'ja', 'fr', 'zh-Hans']

for main_category, sub_categories in categories.items():
    if main_category not in user_follow_categories: continue
    for sub_category in sub_categories:
        print(f"[{main_category}] {sub_category} ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        articles = fetch_news(sub_category)
        if not articles:
            print("  -> ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            continue
        processed_articles = []
        for article in articles:
            print(f"  - '{article['title'][:30]}...' GPT í‰ê°€ ë° ë²ˆì—­ ì¤‘...")
            evaluation_text = gpt_evaluate(article['content'], user_selected_sources)
            time.sleep(1)
            summary_match = re.search(r'1\)(.*?)(?=2\)|\Z)', evaluation_text, re.DOTALL)
            reliability_match = re.search(r'ì‹ ë¢°ë„:\s*(ë†’ìŒ|ë³´í†µ|ë‚®ìŒ)', evaluation_text)
            summary_text = summary_match.group(1).strip() if summary_match else "ìš”ì•½ ì •ë³´ ì—†ìŒ"
            reliability = reliability_match.group(1).strip() if reliability_match else "ì•Œ ìˆ˜ ì—†ìŒ"
            azure_translations = translate_with_azure(f"{article['title']}\n{summary_text}", target_languages)
            translated_titles = {lang: trans_text.split('\n', 1)[0] for lang, trans_text in azure_translations.items()}
            reliability_class = {"ë†’ìŒ": "high", "ë³´í†µ": "medium"}.get(reliability, "low")
            summary_html = f"<div class='summary'>{summary_text.replace('\n', '<br>')}<span class='reliability {reliability_class}'>ì‹ ë¢°ë„: {reliability}</span></div>"
            article_data = {
                'link': article['link'], 'image_url': article['image_url'], 'source': article['source'], 'date': article['date'],
                'translations': {
                    'ko': {'title': article['title'], 'summary_html': summary_html},
                    'en': translated_titles.get('en', "ë²ˆì—­ ì˜¤ë¥˜"), 'ja': translated_titles.get('ja', "ë²ˆì—­ ì˜¤ë¥˜"),
                    'fr': translated_titles.get('fr', "ë²ˆì—­ ì˜¤ë¥˜"), 'zh-Hans': translated_titles.get('zh-Hans', "ë²ˆì—­ ì˜¤ë¥˜")
                }
            }
            processed_articles.append(article_data)
        save_news_with_translations(main_category, sub_category, processed_articles)
        print(f"  -> {len(processed_articles)}ê°œ ë‰´ìŠ¤ ì €ì¥ ì™„ë£Œ.")
print("\nğŸ‰ ëª¨ë“  ë‰´ìŠ¤ ìˆ˜ì§‘, í‰ê°€ ë° ë²ˆì—­ ì™„ë£Œ!")